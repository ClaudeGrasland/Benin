---
title: "ECOLE D'ETE CIST 2022"
subtitle: "Inventaire préparatoire des données"
date: "`r Sys.Date()`"
author: 
 - name: Claude Grasland
   affiliation: Université de Paris (Diderot), UMR 8504 Géographie-cités, FR 2007 CIST
image: "data/figures/logoEE.png"   
logo: "data/figures/logoEE.png"  
output:
  rzine::readrzine:
    highlight: kate
    number_sections: true
csl: Rzine_citation.csl
bibliography: biblio.bib
nocite: |
  @*
link-citations: true
#licence: "[![licensebuttons by-sa](https://licensebuttons.net/l/by-sa/3.0/88x31.png)](https://creativecommons.org/licenses/by-sa/4.0)"
#giturl: "[![Code source on GitHub](https://badgen.net/badge/Code%20source%20on%20/GitHub/blue?icon=github)](xxx)"
#doi: "[![DOI:xxx](https://zenodo.org/badge/DOI/xxx.svg)](https://doi.org/xxx)"
---


```{r setup, include=FALSE}

library(knitr)
library(rzine)
library(sf)
library(leaflet)
library(FactoMineR)
library(mapsf)
library(data.table)
library(tidyr)
library(dplyr)
library(ggplot2)
library(cowplot)
library(mapview)
library(DT)
library(stargazer)
library(wbstats)
library(rnaturalearth)
library(rnaturalearthdata)

library(quanteda)
library(tidytext)
library(plotly)
library(RColorBrewer)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=FALSE,
               prompt=FALSE,
               tidy=FALSE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               class.source="bg-info",
               class.output="bg-warning")

# opts_knit$set(width=75)
```

# INTRODUCTION

L'objectif de ce document est d'examiner quelles sources en accès libre pourraient être utilisées pour la préparation de l'école d'été du CIST afin de construire des modules pédagogiques qui seront mis en ligne à une date mais pourront ensuite être régulièrement mises à jour.

Nous avons choisi de nous focaliser sur l'exemple des données relatives au **Benin** puisque ce pays accueillera la seconde phase du projet,mais l'idée est évidemment de pouvoir constituer des tableaux comparables dans chacun des pays concernés (Togo, Côte d'Ivoire, Niger, Burkina faso, Mali, Sénégal, ...) au prix d'éventuelles adaptations.

Les tableaux de données sont sélectionnées prioritairement sur leurs **qualités pédagogiques** c'est-à-dire leur intérêt pour apprendre les méthodes de statistique, cartographie ou analyse spatiale. Mais il est évidemment souhaitable que ces données possèdent également un **intérêt thématique** et que le formateur puisse proposer des interprétations des résultats. Et il faut évidemment éviter que ces données comportent des erreurs ou véhiculent des informations fausses.

C'est la raison pour laquelle la sélection initiale effectuée par un enseignant-chercheur éloigné du terrain devra impérativement être validée par  des spécialistes du pays concerné, aussi bien en termes pédagogiques que thématiques.

Ce document de travail est organisé en trois parties  :

- **les données infranationales** permettent d'analyse les variations internes du territoire du Benin que ce soit dans le cadre de mailles administratives (régions, départements, communes), d'unités ponctuelles (villes, marchés, ...) ou de grilles de collecte (carroyage, données satellitaires, ...)

- **les données supranationales** permettent a contrario de positionner le Benin considéré comme une seule entité vis à vis d'autres pays à l'échelle d'une région (e.g. CEDEAO), d'un continent (Afrique), voire du Monde entier.

- **les données non conventionnelles** correspondent à d'autres sources de données ne relevant pas de la statistique publique et qui sont désignés en anglais sous le nom de *soft data* souvent issues du web ou des réseaux sociaux. On se limite ici à un exemple,les flux RSS émis par deux journaux du Bénin, mais ce type d'analyse pourrait évidemment être fait sur des données twitter, facebook, etc. 

Ce document n'est évidemment pas exhaustif et devra être complété par d'autres chercheurs, notamment s'agissant des données d'enquête (e.g. EDS) ou des relevés de terrain (GPS, kobo toolbox, ...)

# DONNEES INFRANATIONALES



## La source HDX (Humanitarian Data Exchange) 

Il s'agit d'une source majeure qui semble utilisable dans tous les pays. 


Le sigle **OCHA** désigne le **United Nation Office for the Coordination of Human Affairs** dont la vocation est de fournir une aide dans les situations d'urgence. Sa mission qui est expliquée en détail sur [son site web](https://www.unocha.org/about-ocha) comporte de nombreux volets. Cette agence est organisée sur un plan régional et comporte notamment une délégation spécialisée dans les pays d'Afrique Centrale et de l'Ouest appelée **OCHA-ROWCA** qui couvre l'ensemble despays ciblés par l'école d'été du CIST.



Pour mener à bien ses actions l'OCHA-ROWCA a développé un grand nombre de bases de données qui sont mises à jour régulièrement avec un accès à première vue facile. Ces données sont accessiblesà travers la plateforme [Humanitarian Data Exchange (HDX)](https://data.humdata.org/) qui semble avoir été mise au point par OCHA. Cette plateforme HDX ne comporte pas seulement des données mais aussi différents outils de visualisation ou d'analyse. 

Cela pourrait donc constituer une source majeure de données pour l'école d'été CIST 2022-2023, mais il faut en faire un inventaire précis. L'objet de cette note est de procéder à quelques explorations préliminaires en prenant comme cible le Bénin. 


Si nous effectuons une requête `Benin`  sur le site de données HDX ce qui conduit à une liste de 142 sources de données provenant soit de OCHA-ROWCA, soit d'autres agences des Nations-Unies (FAO, WorldBank, WolrdPop, ...). On trouve encore plus de sources (environ 200) si on utilise le menu de recherche par pays car le moteur ajoute dans ce cas des bases de données transnationales. 

Le HDX est donc un véritable hub de concentration des données les plus récentes sur les pays qui nousintéressent pour l'école d'été, et pas seulement le Benin.



```{r}
knitr::include_graphics("data/figures/HDX.png")
```

Comme on ne peut tout explorer d'un coup, nous commençons par les données qui viennet à proprement parler d'OCHA-ROWCA. 


Dans les sections suivantes nous proposons de rassembler des tableaux de données harmonisées en vue de l'école d'été CIST 2022. Nous privilégions l'exemple du Bénin qui sera le lieu de la formation. Chaque tableau de données fait l'objet d'un exemple rapide d'application de méthodes statistiques ou carographiques afin de vérifier la fiabilité et la qualité des données 

## Limites administratives 


### Métadonnées

Le dossier des limites administratives du Benin fourni par HDX ([cliquer ici](https://data.humdata.org/dataset/benin-administrative-boundaries)) est accessible au format shapefile et comporte différentes couches correspondant aux différents niveaux administratifs. Sa datation laisse augurer qu'il s'agit de la situation la plus récente: les fichiers datent de juin 2021 mais leur nom comporte la séquence "20190816" qui doit plutôt correspondre au 16 Août 2019. En pratique, les limites sont celles du recensement de 2013. 


**N.B.** nous avons toutefois découvert une erreur importante dans le code de la commune d'Adjara qui est noté *BJ0101* alors que le code exact est *BJ1001*. Nous avons également constaté une variabilité des noms de vraiables désignant les codes dans la base OCHA. C'est pourquoi nous avons préféré harmoniser les fichiers. 

```{r, fig.width=5,fig.cap="Départements et communes du Bénin (Source : OCHA-ROWCA)"}

adm0<-st_read("data/admin/ben_adm0.shp",quiet = TRUE)
adm1<-st_read("data/admin/ben_adm1.shp",quiet = TRUE)
adm2<-st_read("data/admin/ben_adm2.dbf", quiet = TRUE)

```


### Exemple d'application

Pour vérifier la qualité des fichiers nous effectuons un calage sur un fonds de carte OpenStreetMap à l'aide du package `leaflet` : 


```{r, fig.width=5,fig.cap="Superposition des fonds de carte sur OSM"}



map <- leaflet() %>% 
            addTiles() %>%
            setView(lat = 8, lng=2.1, zoom = 6) %>%
              addPolygons(data = adm2,
                          label = ~ADM2_NAME,
                          highlightOptions = TRUE,
                           fill = TRUE,
                           color = "orange",
                           weight = 3 ) %>%
             addPolygons(data = adm1,
                        fill = FALSE,
                        color = "red",
                        weight = 2) %>%
             addPolygons(data = adm0,
                        fill = FALSE,
                        color = "black",
                        weight = 1) 
  

map
```



## Pyramide des âges 2019 

### Métadonnées

Un dossier du HDX [accessible ici](https://data.humdata.org/dataset/benin-administrative-level-0-2-sadd-2019-projected-population-statistics) fournit les populations par âge et par sexe en 2019 au niveau des communes, départements ou pays entier. Il s'agit naturellement d'**estimations** mais les données n'en sont pas moins très utile pour toute analyse travaillant sur les situations présentes. Les classes d'âges sont suffisamment détaillées (tranches de 5 ans de 0 à 80 ans) pour procéder à des analyses démographiques intéressantes. 


### Exemple d'application

A titre de vérification, on réalise une analyse factorielle des correspondances AFC sur la pyramide des âges des 77 communes


```{r, fig.cap="AFC sur les structures par âge et sexe des communes du Benin en 2019 (Source : OCHA-ROWCA)"}
don<-read.table("data/pop2019/ben_adm2_pop.csv", sep=";",header=T, encoding = "UTF-8")
don2<-don[,c(11:44)]
row.names(don2)<-don$ADM2_PCODE

afc<-CA(don2,ncp = 10,graph = FALSE)
library(explor)
res <- explor::prepare_results(afc)
explor::CA_var_plot(res, xax = 1, yax = 2, lev_sup = FALSE, var_sup = FALSE,
    var_sup_choice = , var_hide = "Row", var_lab_min_contrib = 0, col_var = "Position",
    symbol_var = NULL, size_var = NULL, size_range = c(10, 300), labels_size = 10,
    point_size = 56, transitions = TRUE, labels_positions = "auto", xlim = c(-0.234,
        0.332), ylim = c(-0.187, 0.38))
```

On enchaîne par une CAH (sur les axes de l'AFC) :

```{r, fig.cap = "Type de structures par âge et sexe des communes du Benin en 2019 (source : OCHA)" }
cah <- HCPC(afc,nb.clust = 5,graph = FALSE)
plot.HCPC(cah,choice = "tree")

```



```{r, fig.cap = "Position des classes de la CAH sur les axes de l'AFC (source : OCHA)"}
plot.HCPC(cah,choice = "map", label.cex=0.3)
```

Il reste à visualiser les classes. On crée pour cela une fonction assez complexe de comparaison entre le profil de celles-ci et le profil du Bénin, inspirée d'un programme trouvé sur un [forum du CIRAD](http://forums.cirad.fr/logiciel-R/viewtopic.php?t=9381) 

```{r, fig.height=8, echo=FALSE, fig.cap = "Profils moyens des classes (source : OCHA)"}
pyrclas <- function(numclas=1) {  
ages<- data.table(cah$data.clust)
fem<-ages[,list(F_00_04 = sum(F_00_04),
                 F_05_09 = sum(F_05_09),
                 F_10_14 = sum(F_10_14),
                 F_15_19 = sum(F_15_19),
                 F_20_24 = sum(F_20_24),
                 F_25_29 = sum(F_25_29),
                 F_30_34 = sum(F_30_34),
                 F_35_39 = sum(F_35_39),
                 F_40_44 = sum(F_40_44),
                 F_45_49 = sum(F_45_49),
                 F_50_54 = sum(F_50_54),
                 F_55_59 = sum(F_55_59),
                 F_60_64 = sum(F_60_64),
                 F_65_69 = sum(F_65_69),
                 F_70_74 = sum(F_70_74),
                 F_75_79 = sum(F_75_79),
                 F_80plus = sum(F_80plus)
                 ),list(clust)]

femtot<-fem[,list(F_00_04 = sum(F_00_04),
                 F_05_09 = sum(F_05_09),
                 F_10_14 = sum(F_10_14),
                 F_15_19 = sum(F_15_19),
                 F_20_24 = sum(F_20_24),
                 F_25_29 = sum(F_25_29),
                 F_30_34 = sum(F_30_34),
                 F_35_39 = sum(F_35_39),
                 F_40_44 = sum(F_40_44),
                 F_45_49 = sum(F_45_49),
                 F_50_54 = sum(F_50_54),
                 F_55_59 = sum(F_55_59),
                 F_60_64 = sum(F_60_64),
                 F_65_69 = sum(F_65_69),
                 F_70_74 = sum(F_70_74),
                 F_75_79 = sum(F_75_79),
                 F_80plus = sum(F_80plus)
                 )]



hom<-ages[,list(M_00_04 = sum(M_00_04),
                 M_05_09 = sum(M_05_09),
                 M_10_14 = sum(M_10_14),
                 M_15_19 = sum(M_15_19),
                 M_20_24 = sum(M_20_24),
                 M_25_29 = sum(M_25_29),
                 M_30_34 = sum(M_30_34),
                 M_35_39 = sum(M_35_39),
                 M_40_44 = sum(M_40_44),
                 M_45_49 = sum(M_45_49),
                 M_50_54 = sum(M_50_54),
                 M_55_59 = sum(M_55_59),
                 M_60_64 = sum(M_60_64),
                 M_65_69 = sum(M_65_69),
                 M_70_74 = sum(M_70_74),
                 M_75_79 = sum(M_75_79),
                 M_80plus = sum(M_80plus)
                 ),list(clust)]

homtot<-hom[,list(M_00_04 = sum(M_00_04),
                 M_05_09 = sum(M_05_09),
                 M_10_14 = sum(M_10_14),
                 M_15_19 = sum(M_15_19),
                 M_20_24 = sum(M_20_24),
                 M_25_29 = sum(M_25_29),
                 M_30_34 = sum(M_30_34),
                 M_35_39 = sum(M_35_39),
                 M_40_44 = sum(M_40_44),
                 M_45_49 = sum(M_45_49),
                 M_50_54 = sum(M_50_54),
                 M_55_59 = sum(M_55_59),
                 M_60_64 = sum(M_60_64),
                 M_65_69 = sum(M_65_69),
                 M_70_74 = sum(M_70_74),
                 M_75_79 = sum(M_75_79),
                 M_80plus = sum(M_80plus)
                 )]



# Creation de la base "donnee1"
donnee1<-c(1:17)
donnee1<-as.data.frame(donnee1)
donnee1$age<-c("0-4","5-9","10-14","15-19","20-24","25-29","30-34","35-39",
               "40-44","45-49","50-54","55-59","60-64","65-69","70-74","75-79","80 et +")
donnee1$nbrM<-as.numeric(hom[numclas,2:18])
donnee1$nbrM<-100*donnee1$nbrM/sum(donnee1$nbrM)
donnee1$nbrF<-as.numeric(fem[numclas,2:18])
donnee1$nbrF<-100*donnee1$nbrF/sum(donnee1$nbrF)
Classe = donnee1

donnee2<-c(1:17)
donnee2<-as.data.frame(donnee2)
donnee2$age<-c("0-4","5-9","10-14","15-19","20-24","25-29","30-34","35-39",
               "40-44","45-49","50-54","55-59","60-64","65-69","70-74","75-79","80 et +")
donnee2$nbrM<-as.numeric(homtot)
donnee2$nbrM<-100*donnee2$nbrM/sum(donnee2$nbrM)
donnee2$nbrF<-as.numeric(femtot)
donnee2$nbrF<-100*donnee2$nbrF/sum(donnee2$nbrF)
Benin = donnee2

# Reformat data
donnee <- bind_rows(
  Classe %>% mutate(data = "Classe", Classe = NULL),
  Benin %>% mutate(data = "Benin", Benin = NULL)
) %>%
  mutate(age = factor(x = age, levels = unique(age))) %>%
  gather(key = "label", value = "nombre", nbrM, nbrF) %>%
  mutate(
    sex = factor(x = unname(c("nbrM" = "Hommes", "nbrF" = "Femmes")[label]), levels = c("Femmes", "Hommes")),
    direction = c("nbrM" = -1, "nbrF" = 1)[label],
    alpha = data
  )

g <- ggplot() +
  theme_classic() +
  geom_bar(
    data = filter(donnee, data=="Benin"),
    aes(x = age, y = nombre*direction, fill = sex, group = data, alpha = alpha),
    stat = "identity",
    width = 0.90
  ) +
  geom_bar(
    data = filter(donnee, data=="Classe"),
    aes(x = age, y = nombre*direction, fill = sex, group = data, alpha = alpha),
    stat = "identity",
    width = 0.45
  ) +
  geom_hline(yintercept = 0) +
  coord_flip() +
  scale_y_continuous(labels = function(x){paste0(abs(x), "%")}) +
  scale_alpha_manual(values = c(0.2, 1), breaks = c("Benin", "Classe")) +
  scale_fill_discrete(drop = FALSE) +
  labs(x = "Age", y = NULL, fill = "Sexe")

return(g)

}

leg <- get_legend(pyrclas(1))
p1<-pyrclas(1)+ theme(legend.position = "none")
p2<-pyrclas(2)+ theme(legend.position = "none")
p3<-pyrclas(3)+ theme(legend.position = "none")
p4<-pyrclas(4)+ theme(legend.position = "none")
p5<-pyrclas(5)+ theme(legend.position = "none")



plot_grid(p1, p2, p3, p4, p5, leg,labels = c("1", "2", "3", "4", "5",""), align = "hv",ncol=2)

```

- la **classe 1** : correspond à une population beaucoup plus jeune que la moyenne du pays avec un excédent de 0-9 ans et un déficit de 30 ans et plus. C'est la partie du pays où la transition démographique semble être la plus tardive
- la **classe 2** : présente également un profil de population jeune avec un excédent de 5-14 ans, mais se caractérise par une réduction relative des 0-4 ans qui témoigne soit d'une baisse de la fécondité, soit d'un exode rural des jeunes ménages avec enfants.
- la **classe 3** correspond au profil moyen du pays.
- la **classe 4** est caractérisée par un excédent de personnes âgées et un déficit à la fois de jeunes enfants et de jeunes adultes. Elle correspond à des zones de vieillissment relatif de la population, soit sous l'effet de l'exode rural, soit en raison d'une baisse de la fécondité.
- la **classe 5** est caractérise par une forte surreprésentation des jeunes adultes et des persones âgées, associée à un net déficit des enfants jeunes de 0 à 14 ans. Il s'agit vraisemblablement de zones urbaines caractérisées par une fécodité plus tardive et une meilleure espérance de vie.  

On peut visualiser la distribution de ces cinq classes sur une carte : 


```{r, echo=FALSE, fig.cap = "Distribution spatiale des classes (source : OCHA)"}

par(mfrow = c(1,1))


### Carte classique

adm2$clust<-cah$data.clust$clust

# set theme
mf_theme("candy")
# plot administrative status
mf_map(
  x = adm2, 
  var = "clust", 
  type = "typo",
  pal = c("red", "orange","yellow","green","blue"), 
  lwd = .5,
  leg_pos = "topright",
  leg_title = ""
)

# layout
mf_layout(title = "Carte géométrique",  
          scale = "FALSE",
          credits = paste0("Sources: OCHA, 2019\n",
                            "Ecole d'été CIST 2022"))




```




## Alphabetisation et langues (2013)


### Métadonnées 

Ce fichier qui est accessible sur HDX en suivant [ce lien](https://data.humdata.org/dataset/benin-languages) a été mis à disposition par l'organisation [Translators without borders](https://translatorswithoutborders.org/) mais l'analyse des métadonnées montre qu'il s'agit en fait de données extraites du recensement géénral de population du Bénin de 2013 :


> Created by Translators without Borders,  Uploaded on Jun-21, Version 1

>**Notes and caveats**

>  + All data is drawn from government survey results and is subject to any associated limitations or distortions present in the source data.
>  + Literacy was measured as the ability to read and write in any language.
Languages with population shares less than 0.01% or under 1,000 people (whichever is greater) have been aggregated into the “Other” field.
>  + All decimal values have been rounded to a maximum of 3 decimal places. As a result language shares may not total 100%.
>  + Empty values represent non-existent data and should not be treated as zero values.
Data is available under an Attribution NonCommercial ShareAlike 4.0 International license (CC BY NC SA 4.0)

>**Copyright and terms of use**

>  + You are free to share and adapt the data subject to requirements for attribution and non commercial use. 
>  + Any derivative work must be distributed under the same license as the original. Full terms at 
https://creativecommons.org/licenses/by nc sa/4.0/


Les fichiers fournies comportaient de petites erreurs ou des difficultés de formatage rendant compliquée leur importation dans R. Il a donc fallu les corriger un peu avant de les utiliser. Une fois cela effectué, on a vérifié que les données pouvaient se cartographie en effectuant une jointure avec le fichier administratif au niveau communal.  

**N.B.** nous avons retrouvé l'erreur  dans le code de la commune d'Adjara qui est noté *BJ0101* alors que le code exact est *BJ1001*. Nous avons donc reconstruit les fichiers en les harmonisant avec les fonds de carte. 




### Exemple d'application

Le fichier comporte un ensemble de variables concernant le niveau d'alphabétisation des hommes et des femmes de plus de 6 ans en 2013. Voici à titre indicatif les valeurs par département et pour l'ensemble du pays :  


```{r}
dep<-read.table("data/langues/ben_lang_adm1_correct.csv", sep=";",header =TRUE, dec=",")
dep<-dep[,c(1:2,77:82)]
kable(dep)
```

Ce tableau se prêtera particulièrement bien à des exercices d'initiation à la statistiques et la cartographie sous R puisqu'il comporte à la fois des stocks (population par sexe), des taux (% de population alphabétisée par sexe) et une possibilité de les combiner pour retrouver le nombre de personnes alphabétisée. Il s'inscrit par ailleurs pleinement dans la thématique des inégalités territoriales. 

On peut à titre d'exemple construire une cartographie du niveau d'alphabétisation des hommes et des femmes par département du Bénin en 2013 (carte choroplèthe) et superposer par dessus les effectifs correspondants d'hommes et de femmes alphabétisés. 



```{r, echo=FALSE, fig.cap = "Alphabétisation des hommes et femmes du Bénin par département en 2013 (source : RP Bénin 2013, via OCHA)"}

# calcul du nombre de pers. alphabétisées par sexe
dep$literacy_female_pop <- dep$literacy_female*dep$pop_female
dep$literacy_male_pop <- dep$literacy_male*dep$pop_male

# Chargement de la géométrie
map <- st_read("data/admin/ben_adm1.shp", quiet=TRUE)  %>% select(ADM1_CODE)     

# Jointure
mapdep<-merge(map,dep,by="ADM1_CODE")

par(mfrow = c(1,2))


mf_theme("darkula")
mapdep %>% 
  mf_map("literacy_female", 
         type = "choro",
          breaks = c(0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9),
         leg_title = "%", 
         leg_pos = "right",
         ) %>%
  mf_map("literacy_female_pop", 
         type = "prop",
         leg_title = "effectif", 
         leg_pos = "left",
         inches=0.1
         ) 

   mf_layout(title = "Femmes", 
          credits = "EE Cist 2022, Source : RP 2013",
          scale = FALSE,
          arrow = FALSE)

mf_theme("darkula")
mapdep %>% 
  mf_map("literacy_male", 
         breaks = c(0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9),
         type = "choro",
         leg_title = "%", 
         leg_pos = "right"
         ) %>%
  mf_map("literacy_male_pop", 
         type = "prop",
         leg_title = "effectif", 
         leg_pos = "left",
         inches=0.1
         ) 
   mf_layout(title = "Hommes", 
          credits = "EE Cist 2022, Source : RP 2013",
          scale = FALSE,
          arrow = FALSE)


```

On voit apparaître alors une assez forte corrrélation entre les deux distributions, même si les femmes sont moins alphabétisés que les hommes. Ce qui suggère un exercice d'apprentissage de la régression linéaire entre les deux variables.

```{r}
mod.reg <- lm(dep$literacy_female~dep$literacy_male)
#stargazer(mod.reg,type = "html")
ggplot(data = dep) + aes(x=100*literacy_male, y = 100*literacy_female, label = ADM1_NAME) +
                     geom_point(col="red") + 
                     geom_smooth(method="lm") +
                     geom_text(cex=3, nudge_y=2) +
                     scale_x_continuous("Taux d'alphabétisation des hommes (%)") +
                     scale_y_continuous("Taux d'alphabétisation des femmes (%)") +
                     ggtitle("Inégalités d'alphabétisation des départements du Benin par sexe",
                             subtitle = "Source : RP 2013 via OCHA")
```



Les résultats mettent en évidence le fait que l'alphabétisation des femmes au Bénin en 2013 est en moyenne inférieure de 20% à celle des hommes (le paramètre b de la régression est non significatif) et la relation est statistiquement très significative (r2  = 93 \%, p < 0.001). Il existe notamment des résidus c'est-à-dire des départements où la scolarisation des femmes estimée en fonction de celle des hommes est plus forte que prévue (*Alibori, Borgou, Littoral*) ou plus faible que prévue (*Plateau, Couffo, Mono*). 

Le même modèle appliqué au niveau des communes permettrait une analyse plus fine des résultats à travers une carte des résidus. On pourrait aussi imaginer une analyse mutiscalaire croisant les écarts des communes à la moyenne du pays, de leur département et des communes voisines à l'aide du package `mta`. Bref, il s'agit sans nul doute d'un excellent exemple pédagogique pour l'école d'été CIST2022.


## Prix de la nourriture sur les marchés (2002-2021)

### Métadonnées

Ce fichier qui est accessible sur HDX en suivant [ce lien](https://data.humdata.org/dataset/wfp-food-prices-for-benin) a été mis à disposition par  [World Food Programme](https://www.wfp.org/). Cette base qui semble avoir été interrompue présente l'avantage d'offrir une grande profondeur historique et une résolution spatialetrès précise puisque les prix sont localisés par marché et par semaine.

>This no longer updated dataset contains Global Food Prices data from the World Food Programme covering foods such as maize, rice, beans, fish, and sugar for 76 countries and some 1,500 markets. It is updated weekly but contains to a large extent monthly data. The data goes back as far as 1992 for a few countries, although many countries started reporting from 2003 or thereafter.


Le fichier contenant les coordonnées de latitude et de longitude des marchés, il peut théoriquement être cartographié facilement à l'aide d'un outil de cartographie dynamique tel que `mapview`. Mais il peut aussi faire l'objet de multiples agrégations spatiales, temporelles, par produit. Reste évidemment à en apprécier la qualité ...

Noter qu'il est nécessaire d'**éliminer la seconde ligne du tableau initial** sous excel pour pouvoir ensuite charger correctement le fichier. Il est également préférable de lire le fichier avec la fonction *fread()* du package `data.table` faut de quoi on peut avoir des problèmes d'encodage plus ou moins délicats à résoudre. 


```{r}
dt<-fread("data/foodprices/wfp_food_prices_ben.csv")
kable(head(dt))
```

On peut assez facilement spatialiser le fichier qui comporte en tout 51 marchés assez bien répartiis sur l'ensemble du pays comme le montre la carte ci-desous :


```{r}
loc<-dt %>% filter(duplicated(market)==FALSE) %>% select(c(2,3,4,5,6)) %>% arrange (admin1,admin2, market)
markets <- st_as_sf(loc, coords = c("longitude","latitude"))
st_crs(markets)<- 4326
#mapview(markets)


adm0<-st_read("data/admin/ben_adm0.shp",quiet = TRUE)
adm1<-st_read("data/admin/ben_adm1.shp",quiet = TRUE)
adm2<-st_read("data/admin/ben_adm2.shp", quiet = TRUE)

par(mar=c(0,0,0,0))
plot(adm2$geometry,col="gray90", border = "gray20",lwd=0.5)
plot(adm1$geometry,col=NA, border = "black",lwd=1, add=TRUE)
plot(adm0$geometry,col=NA, border = "black",lwd=2, add=TRUE)
plot(markets$geometry,col="red", bg="yellow",pch=21,add=TRUE,cex=0.7)
```

### Exemple d'application

Cette base de données **multidimensionelle** peut servir d'exemple pour de nombreux exercices d'apprentissage de la statistique et de la cartographie sous R. Le prix des produits alimnentaires dépend en effet :

- du type de produit (QUOI) ?
- de la date d'achat (QUAND) ?
- du lieu d'achat (OU ?)

Pour chacune des dimensions, on peut réaliser des procédures **d'agrégation** des valeurs observées ou **d'estimation** des valeurs manquantes. A titre d'exemple, on extrait de la base de donnée l'ensemble des lignes qui concernent le riz

```{r}
riz<- dt %>% filter(substr(commodity,1,4)=="Rice") %>% 
               select(when = date, 
                      where1 = admin1,
                      where2 = admin2,
                      where3 = market,
                      what = commodity,
                      cost = usdprice )
kable(head(riz))
```


On peut calculer le prix median du riz sur l'ensemble des points de sondage disponibles en fonction de la catégorie de riz : 

```{r}
tab<- riz %>% group_by(when, what) %>% summarize(median_cost=median(cost), nbobs = n())
ggplot(tab) + aes(x=when,y=median_cost, color=what, size=nbobs) + geom_point() + 
  scale_x_continuous("Données de prix mensuelles") + 
  scale_y_continuous("Prix médian d'un kilog de riz (en $)")+
  ggtitle(label = "Estimation des prix du riz au Bénin",subtitle = "Source : World Food Program (via HDX)")
```

Le graphique montre plusieurs choses :

- les données les plus complètes concernent le riz importé qui augmente fortement au moment de la crise de 2007-2008
- la spatialisation des données n'est pas vraiment possible avant 2015 voire 2018
- il y a de fortes différences de prix selon le type de riz


Effectuons maintenant un zoom sur le priz médian du riz importé en 2020-2021 par régions :

```{r}
tab <- riz %>% filter(when >=as.Date("2020-04-01"), what =="Rice (imported)") %>%
               group_by(when,where1) %>%
               summarize(median_cost=median(cost))
ggplot(tab) + aes(x=when,y=where1, fill=median_cost) +geom_tile() + 
  scale_fill_gradient(low="white", high = "red")+
    scale_x_date("Médiane des valeurs observées par mois") + 
  scale_y_discrete("Département")+
  ggtitle(label = "Prix du riz importé sur les marchés du Benin ($/kg)",subtitle = "Source : World Food Program (via HDX)")
                        


```









## Villages et localités vers 2015


### Métadonnées 

Ce fichier accessible en [cliquant ici](https://data.humdata.org/dataset/benin-settlements) concerne le peuplement, c'est-à-dire apparemment l'inventaire de toutes les localisations avec leur nom et leur position en latitude longitude. Il comporte 6306 entrées. Les fichiers sont datés de 2015. 

```{r, fig.width=5,fig.cap="Village et peuplement au Benin vers 2015 (Source : OCHA-ROWCA)"}
loc<-st_read("data/settlement/ben_plp_NGA.shp", quiet = TRUE)
par(mar=c(0,0,0,0))
plot(adm2$geometry,col="lightyellow", border = "gray20",lwd=0.5)
plot(loc$geometry,col="red",pch=16,cex=0.2, add=T)

```


### Exemple d'application

A titre de vérification de la précision, nous effectuons une superposition sur le fonds de carte OpenStreetMap pour la commune de Ouidah (code BJ0304 ou BEN003004) ou aura lieu l'école d'été du CIST. En cliquant sur les points onpeut comparer leur nom avec celui des localités indiquées par OSM.

```{r, fig.width=5,fig.cap="Projection des données OCHA-ROWCA sur Open Street Map (commune de Ouidah"}
map<-adm2[adm2$admin2Pcod=="BJ0304",]
map<-st_transform(map,4326)

vil<-loc[loc$RowcaCode2=="BEN003004",]
vil<-st_transform(vil,4326)


map <- leaflet() %>% 
            addTiles() %>%
            setView(lat = 6.4, lng=2.1, zoom = 11) %>%
            addPolygons(data = map,
                        fill = FALSE,
                        color = "red",
                        weight = 2) %>%
             addMarkers(data = vil,
                        label = ~FULL_NAME_)

map
```

Il y a à l'évidence des décalages ... Et le contour de la commune lui-même ne semble pas coller exactement avec celui fourni par OSM. Il faudra vérifier laquelle des deux sources est erronée (à moins que ce ne soient les deux ?). Toutefois, la localisation semble rester approximativement juste ...


# DONNEES INTERNATIONALES

Nous allons présenter ici deux packages R qui correspondent à des API permettant de télécharger respectivement des données statistiques de la Banque Mondiale et des fonds de carte Natural Earth. Chacune de ces API a été implémentée sous le forme de package R ce qui permet d'extraire facilement les données, dès lors qu'on dispose d'une connexion internet suffisante. Un intérêt évident de cette approche par API est de pouvoir effectuer facilement des requêtes sur n'importe quelle partie du monde et de pouvoir mettra à jour régulièrement les données au fur et à mesure de leur mise à jour. 

A titre d'exemple, nous allons montrer comment réaliser une carte des émissions de CO2 par habitant des pays de la CEDEAO en 2018. 


## L'API Banque Mondiale 

Supposons que l'on souhaite télécharger la population, le PIB et les émisssions de CO2 des pays du monde de 2000 à 2015. Plutôt que d'aller chercher des fichiers sur un site web, nous allons utiliser une API proposée par la Banque Mondiale qui permet de télécharger les données facilement et surtout de les mettre à jour régulièrement. Pour cela on va installer le package R correspondant à l'API `wbstats` de la Banque mondiale.

https://cran.r-project.org/web/packages/wbstats/vignettes/Using_the_wbstats_package.html



Au moment du chargement du package, il est créé un fichier wb_cachelist qui fournit l'ensemble des donnes disponibles sous la forme d'une liste de tableaux de méta-données.




```{r}
library("wbstats")
cat<-wb_cachelist
str(cat,max.level = 1)
```


### Le tableau "countries"

Il fournit des renseignements de base sur les différents pays, leurs codes, etc.

```{r}
str(cat$countries)
```

Le tableau comporte 304 observation et il mélange des pays (France), des fragments de pays (Réunion) et des agrégats de pays (Europe). Il faudra donc bien faire attention lors de l'extraction à réfléchir à ce que l'on souhaite utiliser. Par exemple, si l'on veut juste les pays :

```{r}
## Programme en langage R_base
# pays<-cat$countries[cat$countries$income_level!="Aggregates",c("iso3c", "country","capital_city","longitude","latitude", "region","income_level")]


## Programme en langage dplyr

pays <- cat$countries %>% 
          filter(income_level !="Aggregates") %>%
          select(iso3c,country, capital_city, latitude, longitude, region, income_level)


kable(head(pays))

```

### Le tableau indicators

Il comporte pas loin de 17000 variables ... Autant dire qu'il est difficile de l'explorer facilement si l'on ne sait pas ce que l'on cherche. 

```{r}
indic<-cat$indicators
dim(indic)
kable(head(indic))
```
#### Recherche du code d'un indicateur

Supposons qu'on recherche les données récentes sur les émissions de CO2. On va utiliser le mot-clé *CO2* pour rechercher les variables correspondantes dans le catalogue à l'aide de la fonction `wbsearch`, ce qui donne 45 réponses 

```{r}
vars <- wbsearch(pattern = "CO2",fields="indicator")
kable(head(vars))
```

On va finalement trouver le code de la variable recherchée

- *EN.ATM.CO2E.KT* : émissions de CO2 en kilotonnes

Les deux autres variables dont nous avons besoin ont pour code 

- *NY.GDP.MKTP.CD* : PIB en parités de pouvoir d'achat
- *SP.POP.TOTL* : Population totale


#### Extraction des métadonnées 

Une fois que l'on pense connaître le code de nos variables, on peut extraire les métadonnés pour vérifier qu'il s'agit bien de ce que l'on cherche, quelle est la source exacte, quelle est l'unité de mesure ...

```{r}
# Programme R-base
meta<-cat$indicators[cat$indicators$indicator_id %in% c("SP.POP.TOTL","NY.GDP.MKTP.CD","EN.ATM.CO2E.KT"),]

# Programme dplyr
meta<-cat$indicators %>%
        filter(indicator_id %in% c("SP.POP.TOTL","NY.GDP.MKTP.CD","EN.ATM.CO2E.KT"))

kable(meta)
```


### L'extraction des données

Elle se fait à l'aide de la fonction `wb_data` qui comporte de nombreuses options. 



#### le paramètre `indicator = `

Ce paramètre permet de choisir les indicateurs à collecter, ce qui suppose que l'on connaisse leur code. Par exemple, supposons que l'on veuille extraire la population et le PIB pour pouvoir calculer ensuite le PIB par habitant

```{r, eval=FALSE}
df   <- wb_data(indicator  = c("NY.GDP.MKTP.CD","SP.POP.TOTL", "EN.ATM.CO2E.KT"))


```

- **commentaire** : Nous obtenons un tableau très grand (> 13000 lignes) qui comporte les valeurs pour toutes les dates disponibles depuis 1960 et pour tous les pays, même si les valeurs sont souvent manquantes. 


### le choix d'une période de temps

#### les paramètres `startdate = ` et `startdate = ` 

Ces deux paramètres permettent de choisir une plage de temps. On peut par exemple décider de ne collecter que les données relatives aux années 2017, 2018 et 2019 

```{r}
df   <- wb_data(indicator  = c("NY.GDP.MKTP.CD","SP.POP.TOTL", "EN.ATM.CO2E.KT"),
                start_date = 2017,
                end_date = 2019)
dim(df)
kable(head(df,6))

```

- **commentaire** : Le tableau ne comporte donc plus que 651 lignes correspondant aux trois dates pour les différents pays du Monde. 


#### Le paramètre `mrv` (most recent value)

Lorsque l'on souhaite juste obtenir les données les plus récentes, on peut remplacer les paramètres `startdate = ` et `startdate = `  par le paramètre `mrv = ` suivit d'un chiffre indiquant le nombre d'années que l'on souhaite à partir de la date la plus récente. Avec mrv=1 on récupère uniquement la dernière année disponible pour au moins l'une des variables.

```{r}
df   <- wb_data(indicator  = c("NY.GDP.MKTP.CD","SP.POP.TOTL","EN.ATM.CO2E.KT"),
                mrv = 1)
dim(df)
kable(head(df,6))
```

L'inconvénient de cette méthode est que cela peut aboutir à un grand nombre de valeurs manquantes si l'une des variables recherchée n'a pas été mise à jour. 
Il est donc préférable de sélectioner une période plus longue mrv=5 et de faire ensuite soi-même le tri

### Le choix des unités géographiques



Le paramètre `country = ` permet de choisir les entités spatiales à collecter, soit sous forme de liste de codes, soit à l'aide de valeurs spéciales. Par défaut; il renvoie la liste de tous les pays, mais on peut se limiter à quelques uns seulement à l'aide de leur nom en anglais (risqué ...) ou de leur code ISO3 (plus sûr)

#### sélection de pays

```{r}
df   <- wb_data(indicator  = c("NY.GDP.MKTP.CD","SP.POP.TOTL","EN.ATM.CO2E.KT"),
                start_date = 2018,
                end_date = 2018,
                country = c("BEN","TGO"))
df$GDP.per.capita <- round(df$NY.GDP.MKTP.CD / df$SP.POP.TOTL,0)
df$CO2.per.capita <- round(1000*df$EN.ATM.CO2E.KT / df$SP.POP.TOTL,2)
kable(head(df,6))
```

- **commentaire** : Il est donc facile de travailler sur un petit nombre de pays que l'on souhaite comparer.

#### Opérateurs spéciaux

Il existe un certain nombre de paramètres spéciaux que l'on peut utiliser à la place de la liste des pays :

- "countries_only" (Default)
- "regions_only"
- "admin_regions_only"
- "income_levels_only"
- "aggregates_only"
- "all"




```{r}
df   <- wb_data(indicator  = c("NY.GDP.MKTP.CD","SP.POP.TOTL","EN.ATM.CO2E.KT"),
                start_date = 2018,
                end_date = 2018,
                country = "regions_only")
df$GDP.per.capita <- round(df$NY.GDP.MKTP.CD / df$SP.POP.TOTL,0)
df$CO2.per.capita <- round(1000*df$EN.ATM.CO2E.KT / df$SP.POP.TOTL,2)
kable(df)
```

- **commentaire** : Nous avons extrait les données par grandes régions du Monde pour l'année 2018

### Le format de sortie du tableau

Il existe deux façons d'extraire un tableau comprenant plusieurs variables ou plusieurs dates, selon que l'on veut un tableau large (wide) ou étroit. On peut régler la sortie à l'aide du paramètre `return_wide` qui est TRUE par défaut mais que l'on peut régler sur FALSE.

#### `return_wide` = FALSE

```{r}
df   <- wb_data(indicator  = c("NY.GDP.MKTP.CD","SP.POP.TOTL","EN.ATM.CO2E.KT"),
                return_wide = TRUE,
                start_date = 2017,
                end_date = 2018,
                country = c("BEN","TGO"))
df
```

#### `return_wide` = FALSE

```{r}
df   <- wb_data(indicator  = c("NY.GDP.MKTP.CD","SP.POP.TOTL","EN.ATM.CO2E.KT"),
                return_wide = FALSE,
                start_date = 2017,
                end_date = 2018,
                country = c("BEN","TGO"))
df[,1:7]
```


## L'API Natural Earth


###  Natural Earth

Nous allons ici utiliser le fonds de carte *Natural Earth* qui est un fonds de carte libre de droit et mis à jour régulièrement. Le site web du projet se situe à l'adresse suivante :

https://www.naturalearthdata.com/

Il indique ses objectifs comme suit : 

> "Natural Earth is a public domain map dataset available at 1:10m, 1:50m, and 1:110 million scales. Featuring tightly integrated vector and raster data, with Natural Earth you can make a variety of visually pleasing, well-crafted maps with cartography or GIS software.[...] Natural Earth was built through a collaboration of many volunteers and is supported by NACIS (North American Cartographic Information Society), and is free for use in any type of project (see our Terms of Use page for more information)."

On peut télécharger les différents fonds de carte sur le site web, mais dans une perspective de mise à jour automatique régulière du fonds de carte il est plus pertinent d'utiliser l'API `rnaturalearth`qui permet d'accéder directement à la plupart des fonds de carte avec juste quelques lignes de code. Il suffit pour cela de commencer par installer et charger le package.

```{r}
library("rnaturalearth")
library("rnaturalearthdata")
```



### le fonds de carte *countries110* (175 unités)

On va télécharger tout d'abord le fonds de carte des pays du Monde avec une forte généralisation des contours `countries110` et le transformer en objet de type spatial feature du package  `sf` du package avant de le visualiser et d' examiner le nombre d'unités 


```{r}
map<-st_as_sf(countries110)
class(map)

ggplot(data = map) +
        geom_sf(fill="lightyellow") +
        theme_bw()


```

Ce fonds de carte comporte 175 unités spatiales, mais de quoi s'agit-il exactement. Les métadonnées associées permettent de se faire une idée plus précise de la nature exacte de ces unités. Prenons pour cela quelques exempes

```{r}
sel<-map[map$adm0_a3 %in% c("FRA", "NCL","ATA","ATF","USA", "PRI","CHN","TWN","MAR", "SAH","CHN","TWN","ISR","PSX"),c("sovereignt","sov_a3","type","admin", "adm0_a3","name","note_adm0","iso_a3","wb_a3")]
kable(sel)
```

Les exemples présentés dans le tableau ci-dessus montrent la complexité du problème de définition et de représentation cartographique des "pays" ou "bouts du monde". Quelques remarques :

1. La *France* (FR1) en tant qu'état souverain regroupe ici cartographiquement la partie métropolitaine du pays et les Départements d'Outre-Mer (Guyane Française, Réunion, Martinique, Guadeloupe) en une seule entité spatiale, mais elle met à part la Nouvelle Calédonie et les îles antarctiques. 
2. *Porto Rico* (PRI) est considéré comme une dépendance des *Etats-Unis* (US1) au même titre que la *Nouvelle Calédonie*(NCL) est considérée comme une dépendance de la *France* (FR1).
3. Le *Sahara occidental* (SAH) est considéré comme une zone *indéterminée* bien qu'il soit occupé par le *Maroc* (MAR).
4. la Palestine (PSX) est considéré comme une zone *disputée* mais rattachée en terme de souveraineté à *Israël* (ISR) et une note précise qu'elle est *partiellement semi-administrée*. Le code sur trois caractères des territoires palestiniens est très variable selon les organisations (PSX, PSE, WBG).
5. *Taïwan* (TWN) est présenté comme un état souverain, mais son code ISO3 est manquant pour la banque mondiale car la Chine refuse de le reconnaître. 
6. Plusieurs états souverains de petite taille sont absents de ce fonds de carte qui ne regroupe que 175 unités soit moins que les 193 pays membres des Nations-Unies. La plupart des îles du Pacifique sont en particulier éliminées car leur surface les rendrait invisible pour le degré de généralisation cartographique adopté. 


###  le fonds de carte *sovereignty110* (171 unités)

On peut obtenir un fonds différent en installant le package complémentaire `rnaturalearthdata` qui permet notamment de distinguer le fonds de carte des *countries* (c'est-à-dire des "bouts du monde" souverains ou non) et des *sovereignty* (c'est-à-dire des états souverains)

```{r}
library(rnaturalearthdata)
map<-st_as_sf(sovereignty110)

ggplot(data = map) +
 geom_sf(fill="lightyellow") +  
  theme_bw()

```

Le fonds de carte permet désormais de récupérer la plupart des pays souverains du Monde, y compris les petits états insulaires du Pacifique, mais il fait disparaître de façon sélective les territoires indéterminés ou disputés. Ainsi, le Sahra Occidental demeure partiellement séparé du Maroc mais les territoires palestiniens sont annexés à Israël ainsi que le plateau du Golan ce qui n'est évidemment pas un choix neutred'un point de vue géoolitique.

```{r}
par(mfrow=c(1,2))

map2<-map %>% filter(sov_a3 %in% c("ISR","JOR","SYR","LBN","EGY"))
ggplot(data = map2) +
    geom_sf(fill=c("gray80","orange","gray80","gray80","gray80")) +
    ggtitle("Limits of Israël") +
  theme_minimal()


map3<-map %>% filter(sov_a3 %in%c("MAR","SAH","DZA","MRT"))
ggplot(data = map3) +
    geom_sf(fill=c("gray70","orange","gray70","lightyellow")) +
    ggtitle("Limits of Morocco") +
  theme_minimal()




```



### Le fonds de carte *countries50* 

On peut également choisir un fonds moins généralisé dans lequel tous les petits pays seront présents

```{r}

map<-st_as_sf(countries50)
ggplot(data = map) +
    geom_sf(fill="lightyellow") +
  theme_bw()

```

Il existe toute une série d'autres fonds de carte dans le package Natural Earth, notamment avec des résolutions plus précises, mais on se limitera ici à l'exploration des fonds de carte utile pour produire des cartes à contour généralisé couvrant le monde entier.


## Exemple d'application

Nous allons essayer de constituer une carte des émissions de CO2 par habitant des pays de la CEDEAO en 2018 basée sur la combinaison des données `wbstats` et du fonds de carte `naturalearth`.

### Etape 1 : récupération des données statistiques


Nous commençons par récupérer les données brutes de population et de CO2 en 2018 pour les pays de la CEDEAO et on y ajoute les latitues et longitues des centres des pays.

```{r}
cedeao<-c("BEN","BFA","CPV","CIV", "GMB","GIN","GNB","GHA","LBR","MLI","NER","NGA","SEN","SLE","TGO")
df   <- wb_data(indicator  = c("SP.POP.TOTL", "EN.ATM.CO2E.KT"),
                return_wide = TRUE,
                start_date = 2018,
                end_date = 2018,
                country = cedeao)
kable(df)

```

Nous renommons les variables pour avoir un tableau plus simple ou la population est en millions d'habitants, les émissions de CO2 en millions de tonnes. On y ajoute l'intensité des émissions en tonnes par habitant.



```{r}

don <-df %>% select(ISO3 = iso3c, NOM = country, POP = SP.POP.TOTL, CO2 = EN.ATM.CO2E.KT) %>%
            mutate(POP = POP/1000000, CO2 = CO2/1000, CO2_hab = CO2/POP)
kable(don,digits = 2)

```





### Etape 2 : Récupération du fonds de carte

on récupère ensuite le fonds de carte en ne gardant que les pays de la CEDEAO

```{r}
cedeao<-c("BEN","BFA","CPV","CIV", "GMB","GIN","GNB","GHA","LBR","MLI","NER","NGA","SEN","SLE","TGO")
map<-st_as_sf(countries110)
map<-st_as_sf(countries110) %>% 
        select(adm0_a3,geometry) %>% 
        rename(ISO3 = adm0_a3) %>%
        filter(ISO3 %in% cedeao)
plot(map$geometry, col="lightyellow")
```



### Etape 3 : Jointure du fonds de carte et des statistiques

```{r}
mapdon <- right_join(don,map) %>% st_as_sf()
kable(head(mapdon))
```

### Etape 4 : Visualisation avec mapsf


```{r}

mf_theme("agolalight")
mapdon %>% 
  mf_map("CO2_hab", 
         "choro",
         breaks="jenks",
         leg_pos="bottomleft",
         leg_title = "en tonnes/hab.")%>%
  mf_map("CO2", 
         "prop",
         col="red",
         leg_pos = "topleft",
         leg_title = "en millions de tonnes")
mf_title("Emissions de CO2 des pays de la CEDEAO en 2018")

```




# DONNEES NON CONVENTIONNELLES

Sous le terme de données *non conventionnelles* on peut regrouper toute une série de bases de données qui ne sont pas issues de la statistique publique mais qui peuvent faire l'objet d'exploitations intéressantes. Cela recouvre notamment les données issues des réseaux sociaux (facebook, twitter, ...) mais aussi les données collaboratives (Open Street Map) où les données d'entreprise privée mises à disposition du public. 

## Données médiatiques

A titre d'exemple de données non conventionnelles, nous proposons d'examiner le cas des flux RSS de deux journaux du Benin, mis à disposition par le site de recherche américain Media Cloud. 


### Préparation des données

#### Importation du fichier csv

Le fichier qui a été préalablement nettoyé coporte 36466 titres de nouvelles de presses publiées entre novembre 2017 et décembre 2021. Sa structure est très simple comme on peut le voir ci-dessous. 

```{r loadcsv, echo=TRUE}
df<-fread("data/corpus/media_benin.csv",encoding = "UTF-8")
kable(head(df))
```



#### Transformation en corpus quanteda

Pour faciliter l'analyse textuelle, le fichier d'origine est transformé en objet de type *corpus* pour être utilisé par le package d'analyse textuelle `quanteda`.  On ajoute au fichier différentes informations sur les périodes de temps pour faciliter les analyses ultérieures.

```{r create quanteda, echo=TRUE}
# library(quanteda)
# Create Quanteda corpus
qd<-corpus(df,text_field = "text")
qd$media<-as.factor(qd$who)
levels(qd$media)<-c("24 Heures","La Nouvelle Tribune")

qd$day     <- as.Date(qd$when)
qd$week    <- cut(qd$day, "weeks", start.on.monday=TRUE)
qd$month   <- cut(qd$day, "months")
qd$weekday <- weekdays(qd$day)


# Add global meta
meta(qd,"meta_source")<-"Media Cloud "
meta(qd,"meta_time")<-"Téléchargé le 18 décembre 2021"
meta(qd,"meta_author")<-"Auteur :  Claude Grasland"
meta(qd,"project")<-"Ecole d'été CIST 2022"

class(qd)
summary(qd,6)
head(qd,6)

```

#### Transformation en tibble

L'objet quanteda étant complexe, on peut décider à tout moment d'opérer une transformation inverse e se servant de la fonction *tidy* du package `tidytext`. 

```{r, echo=TRUE}
td <- tidy(qd)
class(td)
```

#### Variations temporelles

Avant d'analyser les nouvelles, il faut s'assurer que leur production est régulière dans le temps, à la fois pour l'ensemble de la période et au cours des différents jours de la semaine.

```{r, echo=FALSE}

news_weeks <- td %>% group_by(week,media) %>%
                    count(nbnews = n())
p<-ggplot(news_weeks, aes(x=as.Date(week),y=nbnews, col=media))+
   geom_line()+
   geom_smooth(method = 'loess', formula = 'y~x')+
   scale_y_continuous("Nombre de nouvelles", limits = c(0,NA)) +
   scale_x_date("Semaine") +
         ggtitle(label ="Nombre de nouvelles envoyées par flux RSS",
                  subtitle = "1er Nov.2017- 18 Dec.2021")
p
```
- *Commenaire* : Les deux journaux connaissent un régime assez régulier mais en croissance au cours du temps. On note toutefois une interruption de la *Nouvelle Tribune* entre mars et juin 2019. qui pourrait avoir été lien à l'interdiction temporaire du journal. 



```{r news_weekdays_fr, echo=FALSE}
#compute frequencies by weekday
#news_weekdays<-dt[,.(newstot=.N),by=.(weekday,who)]
news_weekdays <- td %>% group_by(weekday,media) %>%
                    count(nbnews = n()) %>% 
                   group_by(media) %>%
                   mutate(pct = 100*nbnews / sum(nbnews))


# Translate weekdays in english and order
news_weekdays$weekday<-as.factor(news_weekdays$weekday)
levels(news_weekdays$weekday)<-c("7.Dimanche","4.Jeudi","1.Lundi","2.Mardi","3.Mercredi","6.Samedi","5.Vendredi")
news_weekdays$weekday<-as.factor(as.character(news_weekdays$weekday))
news_weekdays<-news_weekdays[order(news_weekdays$weekday),]


p<-ggplot(news_weekdays, aes(x=weekday,fill = media, y=pct))+
         geom_bar(position = "dodge", stat="identity")+
         scale_x_discrete("Jour de la semaine")+
         scale_y_continuous("Part des nouvelles (%)", limits = c(0,NA)) +
         ggtitle(label ="Variations hebdomadaires des nouvelles",
                  subtitle = "1er Nov.2017- 18 Dec.2021")
p
```

- *Commenaire* : Les deux journaux connaissent clairement une baisse de leur activité au cours du week-end, l'émission de nouvelles par leur flux RSS  étant plus faible le samedi et le dimanche.



### Taggage géographique

Bien qu'il s'agissent de textes courts, les titres des nouvelles de presse comportent souuvent un grand nombre de noms de lieux qu'il était intéressant d'indetifier sous forme de "tags" c'est-à-dire d'étiquettes indiquant pour chaque nouvelle les lieux mentionnés. 



#### Dictionnaire

On peut par exemple repérer les pays étrangers qui sont cités dans les nouvelles en se servant d'un dictionnaire de mots ou de racines créé dans le cadre de projets de recherche sur les nouvelles internationales (ANR Geomedia, H2020 ODYCCEUS).

```{r load_dict, echo=FALSE}
# Load multilanguage dictionary
dict<-fread("data/corpus/global_state_V2.csv")

# Select french dictionary
dict <- dict[dict$lang=="fr",]

# Exclude Benin
dict<-dict[dict$ISO3 !="BEN",]


head(dict)
```



```{r func_annotate, echo=FALSE}
extract_tags <- function(qd = qd,                      # the corpus of interest
                         lang = "fr",                  # the language to be used
                         dict = dict,                  # the dictionary of target 
                         code = "ISO3" ,                # variable used for coding
                         alias = "x",                   # variable used for alias
                         tagsname = "states",           # name of the tags column
                         split  = c("'","’","-"),       # split list
                         tolow = TRUE  ,                # Tokenize text
                         comps = c("Afrique du sud")  # compounds
                         )
{ 


  
# Tokenize  
x<-as.character(qd)


if(length(split) > 0) { reg<-paste(split, collapse = '|')
                       x <- gsub(reg," ",x)}  
if(tolow) { x <- tolower(x)} 
toks<-tokens(x)

# compounds
if(length(split) > 0) { reg<-paste(split, collapse = '|')
                       comps<- gsub(reg," ",comps)}  
if(tolow)       {comps <- tolower(comps)}  
toks<-tokens_compound(toks,pattern=phrase(comps))

  
# Load dictionaries and create compounds

  ## Target dictionary

labels <-dict[[alias]]
if(length(split) > 0) { reg<-paste(split, collapse = '|')
                       labels<- gsub(reg," ",labels)}  
if(tolow)       {labels <- tolower(labels)}  
toks<-tokens_compound(toks,pattern=phrase(labels))
  
 # create quanteda dictionary
keys <-gsub(" ","_",labels)
qd_dict<-as.list(keys)
names(qd_dict)<-dict[[code]]
qd_dict<-dictionary(qd_dict,tolower = FALSE)

# Identify geo tags (states or reg or org ...)
toks_tags <- tokens_lookup(toks, qd_dict, case_insensitive = F)
toks_tags <- lapply(toks_tags, unique)
toks_tags<-as.tokens(toks_tags)
list_tags<-function(x){res<-paste(x, collapse=' ')}
docvars(qd)[[tagsname]]<-as.character(lapply(toks_tags,FUN=list_tags))
docvars(qd)[[paste("nb",tagsname,sep="")]]<-ntoken(toks_tags)



# Export results
return(qd)
}

### Application de la fonction
frcomps <-c("océan indien","continent américain", "pays américain*")

qd <- extract_tags (qd = qd,
                     lang="fr",
                     dict = dict,
                     code = "ISO3",
                     alias = "x",
                     tagsname = "states",
                     split = c("'","’","-"),
                     comps = frcomps,
                     tolow = TRUE)




```


#### Résultats du taggage

On regarde pour chaque journal la distribution des nouvelles en fonction du nombre de pays étrangers cités. Il apparaît alors clairement que *La Nouvelle Tribune* est beaucoup plus tourné vers l'international (39% des nouvelles citent au moins un pays étrangers) que *24 heures* (moins de 5% des nouvelles citent un pays étranger. 


```{r check_states1_news, echo=FALSE}
x<-100*prop.table(table(qd$nbstates,qd$media),2)
kable(x,digits=3,caption = "Nombre de pays étrangers cités dans les nouvelles")
```

On peut afficher les nouvelles qui affichent un nombre record de pays étrangers dans leur titre. 

```{r check_states2_news, echo=FALSE}
table(qd$nbstates)
check<-corpus_subset(qd,nbstates>3)
x<-data.frame(who=check$who,when = check$when,text=as.character(check),states=check$states,nbstates=check$nbstates)
x<-x[order(x$nbstates,decreasing = T),]
kable(head(x,10))
```






### Taggage thématique

On peut également procéder à un taggae thématique en cherchant à repérer les nouvelles qui correspondent à un sujet dont l'on souhaite étudier la présence dans les médias. A titre d'exemple, nous allons ici essayer de repérer les nouvelles traitant d'épidémies et de pathologies transmissibles

#### Dictionaire

Nous proposons par exemple de partir du dictionaire suivant :

```{r dico pandemic, echo=FALSE}
label <- c("épidémie*", "pandémie*", "crise sanitaire","virus", "vaccin*", "oms", "ébola", "ebola",  "h1n1","sras", "chikungunya", "choléra", "peste", "paludisme", "covid*","coronavir*","ncov*")
code <- c("epi", "epi", "epi","virus", "vaccin","oms", "ebola", "ebola",  "h1n1","sras", "chik", "chol", "pest", "palu","covid","covid","covid")
lang  <- rep("fr", length(label))
dict_pande <- data.frame(code,lang,label)
kable(dict_pande)

frcomps<-c("virus informatique")
```
```{r, echo=FALSE}
### Application de la fonction
frcomps <-c("virus informatique")


qd <- extract_tags (qd = qd,
                     lang="fr",
                     dict = dict_pande,
                     code = "code",
                     alias = "label",
                     tagsname = "pand",
                     split = NULL,
                     comps = frcomps,
                     tolow = TRUE)




```


#### Résultats du taggage


```{r check_pand1_news, echo=FALSE}
x<-100*prop.table(table(qd$nbpand,qd$media),2)
kable(x,digits=3,caption = "Part des nouvelles parlant de pandémies")
```

On peut afficher les nouvelles qui affichent un nombre record de pays étrangers dans leur titre. 

```{r check_pand2_news, echo=FALSE}
table(qd$nbpand)
check<-corpus_subset(qd,nbpand>1)
x<-data.frame(who=check$who,when = check$when,text=as.character(check),pand=check$pand, nbpand=check$nbpand)
x<-x[order(x$nbpand,decreasing = T),]
kable(head(x,10))
```

### Exploration multidimensionnelle

Une fois établi un double taggage géographique (Where) et thématique (What) on peut croiser ces deux dimensions avec le média qui a produit les nouvelles (Who) et le moment où les nouvelles ont été publiés (When). On dispose ainsi de quatre dimensions autour desquelles on peut agréger les nouvelles.


#### Transformation en hypercube

On commence par croiser les quatre dimensions afin de compter le nombre de nouvelles correspondantes et de stocker le résultat dans un objet unique appelé *hypercube*. 

```{r, echo=FALSE}

#' @title create an hypercube
#' @name hypercube
#' @description create a network of interlinked states
#' @param corpus a corpus of news in quanteda format
#' @param who the source dimension
#' @param when the time dimension
#' @param timespan aggreation of time
#' @param what a list of topics
#' @param where1 a list of states
#' @param where2  a list of states


hypercube   <- function( corpus = qd,
                        who = "source",
                        when = "when",
                        timespan = "week",
                        what = "what",
                        where1 = "where1",
                        where2 = "where2")
{


  
# prepare data

  don<-docvars(corpus)
  
  df<-data.table(id     = docid(corpus),
                 who    = don[[who]],
                 when   = don[[when]],
                 what   = don[[what]],
                 where1 = don[[where1]],
                 where2 = don[[where2]])

  # adjust id
 df$id<-paste(df$id,"_",df$order,sep="")
 
# change time span
  df$when<-as.character(cut(as.Date(df$when), timespan, start.on.monday = TRUE))

# unnest where1
  df$where1[df$where1==""]<-"_no_"
  df<-unnest_tokens(df,where1,where1,to_lower=F)
  
# unnest where2
  df$where2[df$where2==""]<-"_no_"
  df<-unnest_tokens(df,where2,where2,to_lower=F) 
  
# unnest what
  df$what[df$what==""]<-"_no_"
  df<-unnest_tokens(df,what,what,to_lower=F) 
  


# Compute weight of news
  newswgt<-df[,list(wgt=1/.N),list(id)]
  df <- merge(df,newswgt, by="id")


# ------------------------ Hypercube creation --------------------#
  
  
# Aggregate
  hc<- df[,.(tags = .N, news=sum(wgt)) ,.(who, when,where1,where2, what)]
  
# Convert date to time
  hc$when<-as.Date(hc$when)
  
# export
  return(hc)
  
}

### Application

hc <-    hypercube( corpus   = qd,
                    who      = "who",
                    when     = "when",
                    timespan = "day",
                    what     = "pand",
                    where1   = "states",
                    where2   = "states")
kable(head(hc))

```

Nous allons ensuite croiser les dimensions deux à deux et procéder à des test statistiques permettant de repérer les anomalies les plus remarquables à l'aide d'un test du chi-2.



```{r, echo=FALSE}
#### ---------------- testchi2 ----------------
#' @title  Compute the average salience of the topic and test significance of deviation
#' @name what
#' @description create a table and graphic of the topic
#' @param tabtest a table with variable trial, success and null.value
#' @param minsamp : Threshold of sample size requested for salience computation
#' @param mintest : Threshold of estimated value requested for chi-square test


testchi2<-function(tabtest=tabtest,
                   minsamp = 20,
                   mintest = 5) 
{
  tab<-tabtest
  n<-dim(tab)[1]
  
  # Compute salience if sample size sufficient (default : N>20)
  tab$estimate <-NA
  tab$salience <-NA
  tab$chi2<-NA
  tab$p.value<-NA
  if (tab$trial > minsamp){ tab$estimate<-round(tab$success/tab$trial,5)
  tab$salience<-tab$estimate/tab$null.value
  
  # Chi-square test if estimated value sufficient (default : Nij* > 5)
  
  for (i in 1:n) {
    if(tab$trial[i]*tab$null.value[i]>=mintest) {  
      test<-prop.test(x=tab$success[i],n=tab$trial[i], p=tab$null.value[i], 
                      alternative = "greater")
      tab$chi2[i]<-round(test$statistic,2)
      tab$p.value[i]<-round(test$p.value,5)
    } 
  }
  }
  return(tab)
}

```




#### Valeur de référence

On commence par calculer la valeur de référence qui est la part des nouvelles relatives à la thématique retenue.

```{r, echo=FALSE}
### ---------------- what ----------------
#' @title  Compute the average salience of the topic
#' @name what
#' @description create a table and graphic of the topic
#' @param hc an hypercube prepared as data.table
#' @param subtop a subtag of the main tag (default = NA)
#' @param title Title of the graphic


what <- function (hc = hypercube,
                  subtop = NA,
                  title = "What ?")
{
 
  
tab<-hc
if (is.na(subtop)){tab$what <-tab$what !="_no_"}else {tab$what <- tab$what == subtop}

tab<-tab[,list(news = sum(news)),by = what]
tab$pct<-100*tab$news/sum(tab$news)

p <- plot_ly(tab,
             labels = ~what,
             values = ~pct,
             type = 'pie') %>%
  layout(title = title,
         xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
         yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))

output<-list("table" = tab, "plotly" =p)

return(output)

}



### Example


res_what <- what(hc = hc,
             subtop = NA,
             title = "Valeur de référence (What)")
res_what$table
res_what$plotly
```

- **Commentaire** : Sur l'ensemble de la période, la part des nouvelles du corpus traitant de pandémies est de 5.5%






#### Variation selon le journal (Who.What)

On examine ensuite si cette valeur de référence est la même dans les différents journaux du corpus.

```{r, echo=FALSE}

#### ---------------- who.what ----------------
#' @title  visualize variation of the topic between media
#' @name who.what
#' @description create a table of variation of the topic by media
#' @param hc an hypercube prepared as data.table
#' @param test : visualize test (TRUE) or salience (FALSE)
#' @param minsamp : Threshold of sample size requested for salience computation
#' @param mintest sample size of estimate for chi-square test (default = 5)
#' @param title Title of the graphic


who.what <- function (hc = hypercube,
                      test = FALSE,
                      minsamp = 20,
                      mintest = 5,
                      title = "Who says What ?")
{
  
  tab<-hc
  {tab$what <-tab$what !="_no_"}
  
  tab<-tab[,list(trial = sum(news),success=round(sum(news*what),0)),by = list(who)]
  ref <-round(sum(tab$success)/sum(tab$trial),4)
  tab$null.value<-ref
  
  tab<-testchi2(tabtest=tab,
                minsamp = minsamp,
                mintest = mintest)
  
  
  
  if (test==FALSE) {tab$index =tab$salience
  tab<-tab[tab$trial > minsamp,]
  mycol<-brewer.pal(7,"YlOrRd")
  } 
  else {tab$index=tab$p.value
  tab<-tab[tab$trial*tab$null.value>mintest,]
  mycol<-brewer.pal(7,"RdYlBu")
  mycol[4]<-"lightyellow"
  }
  
  p <- plot_ly(tab,
               x = ~who,
               y = ~estimate*100,
               color= ~index,
               colors= mycol,
               hoverinfo = "text",
               text = ~paste('Source: ',who,
                             '<br /> Total news  : ', round(trial,0),
                             '<br /> Topic news : ', round(success,0),
                             '<br /> % observed  : ', round(estimate*100,2),'%',
                             '<br /> % estimated : ', round(null.value*100,2),'%',
                             '<br /> Salience : ', round(salience,2),  
                             '<br /> p.value : ', round(p.value,4)),
               type = "bar")  %>%
    layout(title = title,
           yaxis = list(title = "% news"),
           barmode = 'stack')
  
  output<-list("table" = tab, "plotly" =p)
  
  return(output)
  
}


### Example



res_who_what<- who.what(hc=hc, 
                        test = TRUE,
                        minsamp = 5,
                        mintest = 1,
                        title = "Variation selon la source (Who.What)")
res_who_what$plotly

```


- **Commentaire** : Il existe une différence significative entre les deux journaux (p<0.001). Moins de 2% des nouvelles publiées par *24 Heures* parlent de pandémie contre plus 7% des nouvelles publiées par *La Nouvelle Tribune*.


#### Variation temporelle (When.What)

Comment évolue l'intérêt des journaux pour la thématique au cours du temps. La période observée est évidemment marquée par une très forte discontinuité avec l'arrivée du Covid-19. Mais quel est le calendrier précis de son impact sur la presse du Bénin ?

```{r, echo=FALSE}
#### ---------------- when.what ----------------
#' @title  visualize variation of the topic through time
#' @name when.what
#' @description create a table of variation of the topic by media
#' @param test : visualize test (TRUE) or salience (FALSE)
#' @param minsamp : Threshold of sample size requested for salience computation
#' @param mintest sample size of estimate for chi-square test (default = 5)
#' @param title Title of the graphic


when.what <- function (hc = hypercube,
                       test = FALSE,
                       minsamp = 20,
                       mintest = 5,
                       title = "Who says What ?")
{
  
  tab<-hc
  {tab$what <-tab$what !="_no_"}
  
  tab<-tab[,list(trial = sum(news),success=round(sum(news*what),0)),by = list(when)]
  ref <-round(sum(tab$success)/sum(tab$trial),4)
  tab$null.value<-ref
  
  tab<-testchi2(tabtest=tab,
                minsamp = minsamp,
                mintest = mintest)
  
  if (test==FALSE) {tab$index =tab$salience
  tab<-tab[tab$trial > minsamp,]
  mycol<-brewer.pal(7,"YlOrRd")
  } 
  else {tab$index=tab$p.value
  tab<-tab[tab$trial*tab$null.value>mintest,]
  mycol<-brewer.pal(7,"RdYlBu")
  mycol[4]<-"lightyellow"
  }
  
  
  p <- plot_ly(tab,
               x = ~as.character(when),
               y = ~estimate*100,
               color= ~index,
               colors= mycol,
               hoverinfo = "text",
               text = ~paste('Time: ',when,
                             '<br /> Total news  : ', round(trial,0),
                             '<br /> Topic news : ', round(success,0),
                             '<br /> % observed  : ', round(estimate*100,2),'%',
                             '<br /> % estimated : ', round(null.value*100,2),'%',
                             '<br /> Salience : ', round(salience,2),  
                             '<br /> p.value : ', round(p.value,4)),
               type = "bar")  %>%
    layout(title = title,
           yaxis = list(title = "% news"),
           barmode = 'stack')
  
  output<-list("table" = tab, "plotly" =p)
  
  return(output)
  
}


# Modify time period by month
hc2 <- hc %>% mutate(when = cut(when,breaks="month"))


res_when_what<- when.what(hc=hc2, 
                          test=TRUE,
                          minsamp=5,
                          mintest=1,
                          title = "Variation temporelle (When.What)")


res_when_what$plotly
```
- **Commentaire** : Comme dans l'ensemble des pays du Monde, on observe un pic maximal en mars-avril 2020 avec plus de 20% des nouvelles parlant de pandémie contre moins de 1% au cours de l'année précédente. Passée cette période d'intérêt maximal, on observe une stabilisation aux alentours de 7 à 10% des nouvelles publiées. 



#### Variations spatiales (Where.What)

Une pandémie étant un phénomène à la fois spatial et temporel, il est intéressant de repérer les pays cités dans les nouvelles quii parlent de pandémie et de repérer les pays qui y ont été associé le plus significativement. 

```{r, echo=FALSE}

#### ---------------- where.what ----------------
#' @title  visualize spatialization of the topic 
#' @name where.what
#' @description create a table of variation of the topic by media
#' @param hc an hypercube prepared as data.table
#' @param test : visualize test (TRUE) or salience (FALSE)
#' @param minsamp : Threshold of sample size requested for salience computation
#' @param mintest sample size of estimate for chi-square test (default = 5)
#' @param map a map with coordinates in lat-long
#' @param proj a projection accepted by plotly
#' @param title Title of the graphic


where.what <- function (hc = hypercube,
                        test = FALSE,
                        minsamp = 20,
                        mintest = 5,
                        map = world_ctr,
                        proj = 'azimuthal equal area',
                        title = "Where said What ?")
{
  
  tab<-hc
  tab$what <-tab$what !="_no_"
  
  tab<-tab[,list(trial = round(sum(news),0),success=round(sum(news*what),0)),by = list(where1)]
  ref <-round(sum(tab$success)/sum(tab$trial),4)
  tab$null.value<-ref
  
  tab<-testchi2(tabtest=tab,
                minsamp = minsamp,
                mintest = mintest)
  
  
  
  tab<-tab[order(-chi2),]
  
  
  
  if (test==FALSE) {tab$index =tab$salience
  tab<-tab[tab$trial > minsamp,]
  mycol<-brewer.pal(7,"YlOrRd")
  } 
  else {tab$index=tab$p.value
  tab<-tab[tab$trial*tab$null.value>mintest,]
  mycol<-brewer.pal(7,"RdYlBu")
  mycol[4]<-"lightyellow"
  }
  
  
  map<-merge(map,tab,all.x=T,all.y=F,by.x="ISO3",by.y="where1")
  
  
  
  #map2<-map[is.na(map$pct)==F,]
  #map2<-st_centroid(map2)
  #map2<-st_drop_geometry(map2)
  
  
  g <- list(showframe = TRUE,
            framecolor= toRGB("gray20"),
            coastlinecolor = toRGB("gray20"),
            showland = TRUE,
            landcolor = toRGB("gray50"),
            showcountries = TRUE,
            countrycolor = toRGB("white"),
            countrywidth = 0.2,
            projection = list(type = proj))
  
  
  
  p<- plot_geo(map)%>%
    add_markers(x = ~lon,
                y = ~lat,
                sizes = c(0, 250),
                size = ~success,
                #             color= ~signif,
                color = ~index,
                colors= mycol,
                hoverinfo = "text",
                text = ~paste('Location: ',NAME,
                              '<br /> Total news  : ', round(trial,0),
                              '<br /> Topic news : ', round(success,0),
                              '<br /> % observed  : ', round(estimate*100,2),'%',
                              '<br /> % estimated : ', round(null.value*100,2),'%',
                              '<br /> Salience : ', round(salience,2),  
                              '<br /> p.value : ', round(p.value,4))) %>%
    
    layout(geo = g,
           title = title)
  
  
  
  output<-list("table" = tab, "plotly" =p)
  
  return(output)
  
}




### Example

map<-readRDS("data/corpus/world_ctr_4326.Rdata")
hc2<-hc %>% filter(where1 !="_no_", where2 !="_no_")

res_where_what<- where.what(hc=hc2,
                            test=TRUE,
                            minsamp=10,
                            map = map, 
                            mintest =1,
                            title = "Variation spatiale (When.What)")
res_where_what$plotly
```
- **Commentaire** : 6.7%  nouvelles qui citent au moins un pays étranger parlent de pandémie ce qui est plus que la moyenne de référence qui était de 5.5% de nouvelles parlant de pandémie. Certains pays sont plus fréquemment associés à la pandémie que d'autres. Ainsi près de 19% des nouvelles parlant de Chine parlent de pandémie au cours de la période d'observation. Il en va de même pour le Royaume-Uni (15.7%), l'Italie (13.4%), l'Inde (11.9%) ou la Russie (10.9%). La France, en revanche, n'est pas spécialement associée à la pandémie (6.3% des nouvelles) même si elle est le pays le plus cité à ce sujet en nombre absolu. Elle demeure en effet présente dans un grand nombre d'autres sujets (économie, sport, culture, politique, ...). Il en va de même pour les Etats-Unis (6.2%).Finalement, d'autres pays sont significativement peu associés à la pandémie comme la Côte d'Ivoire (2.9%) ou le Nigéria (0%).



#### Prolongements possibles

Au lieu de repérer les pays étrangers, on pourrait essayer de repérer les communes ou département du Bénin qui sont cités dans les nouvelles. Ce qui supposerait de construire un nouveau dictionnaire. On peut évidemment choisir par ailleurs d'autres thèmes que celui des pandémies. 





# Bibliographie {-}

<div id="refs"></div>


# Annexes {-}


## Infos session  {-}

```{r session_info, echo=FALSE}
kableExtra::kable_styling(kable(sessionRzine()[[1]], row.names = F))
kableExtra::kable_styling(kable(sessionRzine()[[2]], row.names = F))
```





## Citation {-}

```{r generateBibliography, echo=FALSE}

cat(readLines('cite.bib'), sep = '\n')

``` 

<br>

## Glossaire {- #endnotes}

```{js, echo=FALSE}

$(document).ready(function() {
  $('.footnotes ol').appendTo('#endnotes');
  $('.footnotes').remove();
});

```


