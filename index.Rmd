---
title: "ECOLE D'ETE CIST 2022"
subtitle: "Inventaire préparatoire des données"
date: "`r Sys.Date()`"
author: 
 - name: Claude Grasland
   affiliation: Université de Paris (Diderot), UMR 8504 Géographie-cités, FR 2007 CIST
image: "data/figures/logoEE.png"   
logo: "data/figures/logoEE.png"  
output:
  rzine::readrzine:
    highlight: kate
    number_sections: true
csl: Rzine_citation.csl
bibliography: biblio.bib
nocite: |
  @*
link-citations: true
#licence: "[![licensebuttons by-sa](https://licensebuttons.net/l/by-sa/3.0/88x31.png)](https://creativecommons.org/licenses/by-sa/4.0)"
#giturl: "[![Code source on GitHub](https://badgen.net/badge/Code%20source%20on%20/GitHub/blue?icon=github)](xxx)"
#doi: "[![DOI:xxx](https://zenodo.org/badge/DOI/xxx.svg)](https://doi.org/xxx)"
---


```{r setup, include=FALSE}

library(knitr)
library(rzine)
library(sf)
library(leaflet)
library(FactoMineR)
library(mapsf)
library(data.table)
library(tidyr)
library(dplyr)
library(ggplot2)
library(cowplot)
library(mapview)
library(DT)
library(stargazer)
library(wbstats)
library(rnaturalearth)
library(rnaturalearthdata)

library(quanteda)
library(tidytext)
library(plotly)
library(RColorBrewer)

## Global options
options(max.print="75")
opts_chunk$set(echo=TRUE,
	             cache=FALSE,
               prompt=FALSE,
               tidy=FALSE,
               comment=NA,
               message=FALSE,
               warning=FALSE,
               class.source="bg-info",
               class.output="bg-warning")

# opts_knit$set(width=75)
```

# INTRODUCTION

L'objectif de ce document est d'examiner quelles sources en accès libre pourraient être utilisées pour la préparation de l'école d'été du CIST afin de construire des modules pédagogiques qui seront mis en ligne à une date mais pourront ensuite être régulièrement mises à jour.

Nous avons choisi de nous focaliser sur l'exemple des données relatives au **Benin** puisque ce pays accueillera la seconde phase du projet,mais l'idée est évidemment de pouvoir constituer des tableaux comparables dans chacun des pays concernés (Togo, Côte d'Ivoire, Niger, Burkina faso, Mali, Sénégal, ...) au prix d'éventuelles adaptations.

Les tableaux de données sont sélectionnées prioritairement sur leurs **qualités pédagogiques** c'est-à-dire leur intérêt pour apprendre les méthodes de statistique, cartographie ou analyse spatiale. Mais il est évidemment souhaitable que ces données possèdent également un **intérêt thématique** et que le formateur puisse proposer des interprétations des résultats. Et il faut évidemment éviter que ces données comportent des erreurs ou véhiculent des informations fausses.

C'est la raison pour laquelle la sélection initiale effectuée par un enseignant-chercheur éloigné du terrain devra impérativement être validée par  des spécialistes du pays concerné, aussi bien en termes pédagogiques que thématiques.


Ce document n'est évidemment pas exhaustif et devra être complété par d'autres chercheurs, notamment s'agissant des données d'enquête (e.g. EDS) ou des relevés de terrain (GPS, kobo toolbox, ...)




# DONNEES DE RECENSEMENT

Les données individuelles sont celles qui offrent les plus riches possibilités de traitement puisqu'il est possible de les agréger selon des dimensions sociales ou spatiale multiples et ainsi produire des tableaux en nombre virtuellement infini. Elles posent cependant des questions de nature différentes selon qu'elles portent sur un échantillon limité ou une population exhaustive.

**Mouftaou Amadou Sanni** souligne l'imporance de l'IREDA qui a réalisé un inventaire des Recensements de Population (RGPHs) et Enquetes Demographiques d'acces libre que l'on peut trouver en suivant   [ce lien]([www.ireda.ceped.org). A partir de ce lien, on peut accéder librement en ligne aux Recensements et Enquetes Démographiques de 23 pays d'Afrique Subsaharienne y compris le Benin.

> "IREDA: Inventaires des Recensements et Enquetes en Afrique" est un projet Exécuté par CEPED/Paris en collaborations avec des intitutions afticaines de collecte de données et de Recherche dont mon Institution le CEFORP/Cotonou. Ce projet a démarré en 2008; il s'est inscrit dans la dynamique de sauvegarde, de conservation et de valorisstion des opérations de collecte sous la forme d'inventaires documentés. Il a été co-financé par Paris21-INSEE-IRD-OIF-CESD "Statistiwues pour Développement 

A partir du site de l'IREDA, on peut notamment accéder à un catalogue d'archives des enquêtes et recensement du Benin en suivant [ce lien](https://nada.insae.bj/index.php/catalog) qui comporte pas moins de 30 fiches descriptives d'enquêtes ou de recensements.

## Données disponibles

Parmi les ressources disponibles, une source incontournable est naturellement le dernier recensement du Benin pour lequel on peut retrouver un grand nombre de fichiers et de publications relatives à la collecte et aux résultats en suivant [ce lien](https://nada.insae.bj/index.php/catalog/18) qui donne accès au téléchargement direct de l'ensemble des archives disponibles. 


```{r, fig.width=8, echo=FALSE}
knitr::include_graphics("data/figures/RGP2013_Benin_catalogue.png")
```


### Métadonnées

Avant toute exploitation statistique des données, il peut être intéressant de montrer aux étudiants tout le détail du processus de réalisation du recensement en s'appuyant à la fois sur la [documentation générale](https://nada.insae.bj/index.php/ddibrowser/18/export/?format=pdf&generate=yes) et sur le [bulletin d'enquête](https://nada.insae.bj/index.php/catalog/18/download/301) proprement dit 

```{r, fig.width=8, echo = FALSE}
knitr::include_graphics("data/figures/RGP2013_Benin_bulletin.png")
```

Mais bien d'autres documents méritent également d'être analysés, notamment ceux relatifs aux procédures de recueil, saisie ou contrôle et ceux relatifs au coût financier de l'opération de recensement. 

### Rapports officiels

La relative ancienneté de ce recensement présente en fait l'avantage de montrer l'ensemble des exploitations officielles qui ont pu en être faites sous l'égide de l'INSAE, à destination des décideurs politiques de tous niveaux mais aussi des entreprises ou des citoyens. Le point commun de la majorité des rapports est de fournir un très grand nombre de tableaux croisés (tris à plat) qui présentent les dénombrement des populations et des ménages à touts les échelles. On notera à cet égard le grand intérêt de la collection des *cahiers de village* qui fournissent pour chacun des douze départements du Bénin des tableaux très précis des situations locales. Malheureusement, seule une partie d'entre eux fournit une cartographie des contours des arrondissements (niveau de découpage infracommunal) et il est en général impossible de localiser les villages qui sont le niveau géographique d'observation le plus fin. Notons enfin que les seules cartes fournies dans ces rapports le sont à l'échelle des départements mais pour ainsi dire jamais à l'échelles des communes et encoremoins à celle des arrondissements.

```{r, fig.width=8, echo =FALSE}
knitr::include_graphics("data/figures/RGP2013_Benin_zonage.png")
```


> **La cartographie locale** semble donc être un enjeu très important pour l'école d'été CIST 2022, mais il ne semble pas forcément évident d'obtenir les contours des unités spatiales de collecte les plus petites (arrondissements, villages) ni même leur géolocalisation. Ces données ne sont en tous les cas pas disponibles sur le site du recensement que nous avons examinée. Elle relèvent probablement d'u autre organisme national (Insitut Géographique National ?) à moins qu'elles ne soient soumises à des règles de confidentialité. 


### Données individuelles

Nous avons découvert à notre grande surprise que les bulletins individuels de recensement des populations et des ménages sont librement accessibles dans le dossier d'archive mis à la disposition du public sous l'appellation "masque de saisie" en suivant [ce lien](https://nada.insae.bj/index.php/catalog/18/download/444). Plus précisément, on obtient un dossier contenant deux fichiers :

- *le fichier de données INSAE_RGPH4_2013.dat* (978 Mo) contient l'enssemble exhaustif des informations sur les points d'enquête (localisation en département, commune, arrondissement, village, bloc, numéro de ménage),  les individus, les ménages et les décès survenus au cours des dernières années.
- *le dictionnaire de métadonnées INSAE_RGPH4_2013.dcf* (103 Ko) décrit de façon précise et détaillée les champs d'information et les codes de chacune des variables.

On peut  lire le fichier en se servant d'un programme R, mais celui-ci est très lourd (près de 20 millions de lignes) et pour les utilisateurs débuutants de R il est  plus simple de l'ouvrir avec le logiciel CSPro qui est l'application qui a été utilisée pour réaliser le recensement au Benin comme dans de nombreux pays africains. 

## Exploitation avec CSPro

> CSPro, abréviation de **Census and Survey Processing System** est un ensemble logiciel comprenant compilateur et différents outils spécialisés dans le développement d’applications de collecte, visualisation, traitement et analyse de données. C’est un logiciel développé par le Census Bureau des États-Unis ainsi que ICF international. Serpro S.A était aussi impliqué dans les premières étapes du développement de CSPro. Le financement de ce programme provient principalement de l’USAID. La première version de CSPro remonte à mai 2000, cependant le développement de ses ancêtres : ISSA et IMPS remonte a plus de 30 années. Les dernières versions du compilateur et des outils sont conçues pour être utilisées sur les ordinateurs ayant comme système d'exploitation Windows 7 et plus. Cependant, les applications implémentées avec CSPro 7+ peuvent être déployées sur n’importe quel ordinateur, tablette, smartphone utilisant Android, Windows UWP ou Windows 7, 8 , 10.  CSPro dispose d’un environnement de développement très simple, mais la puissance de son langage de programmation propre : Le Cspro Programming Language (CPL) lui permet de concevoir des applications complexes et intelligentes susceptibles d’être utilisées dans des domaines variés (Formulaires électroniques pour  Enquêtes et Recensements, implémentation de système d'information sophistiqués : Suivi-Evaluation, Administration et Gestion d’entreprises etc.) Source : [Wikipedia.France](https://fr.wikipedia.org/wiki/CSPro)

Ce logiciel étant gratuit, nous avons pu effectuer un test d'utilisation sur les fichiers du Benin qui s'ouvrent effectivement sans difficultés et peuvent être ainsi décodés rapidement : 


```{r, fig.width=8, echo =FALSE}
knitr::include_graphics("data/figures/RGP2013_Benin_fiche.png")
```

- **Remarques**: Si la disponibilité de ces données semble avérée (lien web public) il faut toutefois faire preuve de prudence dans leur dissémination car il semble possible d'accéder à des données individuelles sensibles ce qui poserait des questions éthiques. Notons pour l'anecdote que l'une des professions possible est "*Président de la République*" (code 001), catégorie qui n'apparaît que pour trois individus sur les dix millionsque compte le Benin... Si on souhaite utiliser ces données dans un cadre pédagogique, il sera sans doute préférable de les anonymiser ce qui constitue en soi un excellent exercice pédagogique. 

### Exemple d'utilisation 


Nous utilisons ici un tableau qui a été réalisé à l'aide de CSPro en croisant les variables "*Département de résidence actuel*" et "*Département de résidence antérieur*". Nous excluons de ce fait les mobilités internationales entrantes et nous ne prenons pas en compte l'ancienneté de la migration qui est variable puisque le recensement demande uniquement le **dernier** changement de domicile. 

```{r, echo=FALSE, }
mig<-read.table("data/rp2013/mobil2013.csv", sep=";",header=T)
tab<-reshape2::dcast(mig,formula = name_i~name_j,value.var = "Fij")
mat<-as.matrix(tab[,-1])
rownames(mat)<-tab[,1]
colnames(mat)<-substr(colnames(mat),1,3)
mat2<-addmargins(mat)
kable(mat2,caption = "Migrations interdépartementales au Bénin  (source : RGPH 2013)")
```

> **Commentaire** : Les lignes correspondent au département de résidence précédente et les colonnes au département de résidence actuelle. La diagonale correspond aux individus présents dans un département n'ayant pas déclaré avoir résidé dans un autre. Les personnes recensées qui déclare avoir résidé à l'étranger sont exclues ce qui explique que le total général soit 9.62 millions d'habitants et non pas de 10 millions. On peut déduire de la somme des lignes et des colonnes (hors diagonale) la somme des départs et des arrivées, ce qui permet par la suite de calculer le volume des migrations et le solde migratoire. On peut enfin examiner pour chaque paire de département la quantité d'échanges bilatéraux ainsi que les transferts nets. Par exemple, on trouve 1319 migrants du département d'Atacora vers celui d'Alibori contre 3246 dans la direction opposée. 

Ce tableau peut donc servir de base à une initiation à l'étude des migrations interne du pays, un premier exercice pouvant consister à en extraire les indicateurs de base sur les départements.


```{r, echo=FALSE}
département<-rownames(mat)
population<-apply(mat,1,FUN="sum")
stables<-diag(mat)
mat0<-mat
diag(mat0)<-0
départs<-apply(mat0,1,FUN="sum")
arrivées<-apply(mat0,2,FUN="sum")
volume<-arrivées+départs
solde<-arrivées-départs
asymétrie<-solde/volume
tab<-data.frame(département,population,stables,départs, arrivées,volume,solde)
tab$tx_mob<-100*volume/population
tab$tx_emi<-100*départs/population
tab$tx_imm<-100*arrivées/population
tab$tx_asy<-100*solde/volume
kable(tab,digits = c(0,0,0,0,0,0,0,2,2,2,2),row.names = F, caption = "Synthèses des migrations inter-départementales du Bénin (source : RGPH, 2013)")
```

> **Commentaire** : Si les chiffres du recensement sont exacts et si nous n'avons pas commis d'erreurs ... le tableau met en évidence des évolutions démographiques très contrastées d'un département à l'autre en termes de mobilité et d'attractivité migratoire. Ainsi le département d'**Alibori** se caractérise apparemment par une très faible mobilité avec un taux d'émigration de 3.3\% et un taux d'immigration de 1.5\% soit un solde migratoire négatif de -15219 habitants. Le département de l'**Atlantique** est quant à lui le plus attractif avec un solde migratoire de +248063 habitant qui s'explique par un taux d'immigration exceptionnel (près du tiers de ses habitants actuels ont résidé antérieurement dans un autre département). A l'inverse, le département du **Littoral** est caractérisé par un solde migratoire de -246908 habitant qui s'explique par une très forte mobilité combinant une assez forte immigration (16\%) et une très forte immigration (45\%). On peut supposer que cela correspond à un phénomène de plaque tournante de la métropole de Cotonou qui attire et renvoie de nombreux migrants de tout le pays tout en s'étalant progressivement au delà des limites étroites de son département vers des périphéries urbaines situées dans les départements de l'Atlantique et de Oueme. Notons pour finir que le département de **Bhttp://127.0.0.1:32566/rmd_output/0/#annexesorgou** connaît également une évolution très positive liée à la croissance de la ville de Parakou, métropole du Nord.  

## Exploitation directe avec R 

L'inconvénient de CSPro est de limiter les possibilités de croisement des données puisque l'outil de base est le tableau à 2 dimensions. Or, il est possible d'obtenir des tableaux beaucoup plus intéressants en utilisant des programmes R qui lisent directement les fichiers de données brutes. A titre d'exemple, nous allons montrer (sans détaillerles programmes) un exemple d'analyse des **trajectoires de mobilités** où nous relevons pour chaque individu trois points dans l'espace-temps.

### Construction du tableau de données

1. Lieu (*loc1*) et (*dat1*) date de naissance de l'individu
2. Lieu (*loc2*) et date de départ (*dat2*) de la dernière résidence si elle est différente de la résidence actuelle
3. Lieu (*loc3*) et date de recensement (*dat3*) dans la résidence actuelle.

On peut en déduire trois autres variables booléennes indiquant l'existence de mobilités passées

- *mob13* : migration durée de vie, si le lieu de naissance est différent du lieu de recensement
- *mob23* : dernière migration, si l'individu a déclaré avoir résidé antérieurement dans un autre lieu
- *mob12* : migration entre la naissance et le dernier lieu de résidence avant le lieu de recensement.

On peut ensuite ajouter au fichier autant de données que l'on souhaite relatives à l'individu ou au ménage. Ici, on a jouté à titre d'exemple uniquement deux variables  :

- *ref* : indique si l'individu concerné est la personne de référence du ménage ou non.
- *sex* : indique si l'individu concerné est un homme ou une femme. 

On choisit ici de retenir les individus qui ont déclaré avoir effectué à la fois une mobilité 1-2 et une mobilité 2-3, ce qui donne un tableau comportant 635051 observations. Il est probablement incomplet car beaucoup d'individus n'ont pas renseigné la seconde localisation ou bien l'ont fait de façon imprécise. Mais il offre néanmoins de nombreuses possibilités d'utilisation pédagogique : 


```{r}
dt<-readRDS("data/rp2013/mobind.RDS")
dt<-data.table(dt)
dt2<-dt[mob12+mob23==2 &is.na(dat2)==F]
kable(head(dt2))

```

> La première ligne du tableau ci-dessus correspond à un homme chef de ménage qui avait environ 34 ans lors du recensement de 2013. Il est né dans la commune de Dassa en 1979, puis a effectué au moins une migration qui l'a amené à un date inconnue à Abomey-Calavi d'où il est parti en 2005 pour Banikoara où il résidait en 2013.
> La seconde ligne du tableau correspond à une femme qui n'est pas chef de ménage et avait environ 30 ans lors du recensement de 2013. Elle est née en 1983 dans la commune de Banikoara puis est partie à une date inconnue à Cotonou qu'elle a quitté en 2008 pour revenir dans sa commune d'origine. 


Comme on peut le voir avec les deux exemples précédents, le tableau permet de repérer soit des **trajectoires complexes** mettant en jeu trois lieux différents, soit des **trajectoires d'aller et retour** concernant des individus qui sont revenus dans leur commune de naissance après avoir séjourné dans une autre commune. 


### Exemple des mobilités depuis ou vers Cotonou

Etudions à tire d'exemple le cas de l'ensemble des personnes qui ont déclarées être présentes dans la commune de Cotonou (code 081) a un moment ou un autre de leur parcours migratoire. On peut distinguer 8 cas


1. Résidence permanente à Cotonou (loc1 = loc2 = loc3 = Cotonou)
2. Arrivée à Cotonou sans relais  (loc1 = X, loc2 = X, loc3 = Cotonou)
3. Arrivée à Cotonou avec relais  (loc1 = X, loc2 = Y, loc3 = Cotonou)
4. Départ puis retour à Cotonou   (loc1 = Cotonou, loc2 = X, loc3 = Cotonou)
5. Départ de Cotonou sans relais  (loc1 = loc2 = Cotonou, loc3 = X)
6. Départ de Cotonou avec relais  (loc1 = Cotonou, loc2 = X, loc3 = Y)
7. Aller à Cotonou puis retour    (loc1 = X, loc2 = Cotonou, loc3 = X)
8. Aller à Cotonou puis départ    (loc1 = X, loc2 = Cotonou, loc3 = Y)


  
  


```{r}
com<-"081"
cas1<-dt[loc1==com][loc2==com][loc3==com][,cas:=1]
cas2<-dt[loc1!=com][loc2==loc1][loc3==com][,cas:=2]
cas3<-dt[loc1!=com][loc2!=loc1][loc2!=com][loc3==com][,cas:=3]
cas4<-dt[loc1==com][loc2!=com][loc3==com][,cas:=4]
cas5<-dt[loc1==com][loc2==loc1][loc3!=com][,cas:=5]
cas6<-dt[loc1==com][loc2!=loc1][loc3!=com][loc3!=loc2][,cas:=6]
cas7<-dt[loc1!=com][loc2==com][loc3==loc1][,cas:=7]
cas8<-dt[loc1!=com][loc2==com][loc3!=loc1][loc3!=com][,cas:=8]
tabcom<-rbind(cas1,cas2,cas3,cas4,cas5,cas6,cas7,cas8)
tabcom$cas<-as.factor(tabcom$cas)
levels(tabcom$cas) <-c("1. Résidence permanente" ,
                       "2. Arrivée sans relais " ,
                       "3. Arrivée avec relais"  ,
                       "4. Aller et retour"      ,
                       "5. Départ sans relais"   ,
                       "6. Départ avec relais"   ,
                       "7. Passage puis retour"  ,
                       "8. Passage puis départ")

res<-tabcom[,.(nb=.N),.(cas)]
restot<-data.table(cas="Total",nb=dim(tabcom)[1])
res2013<-data.table(cas="Résidents 2013 (1+2+3+4)",nb=sum(res[1:4]$nb))

res[c(1,4,5,6),sum(nb)]
natifs<-data.table(cas="Natifs (1+4+5+6)",nb=sum(res[c(1,4,5,6)]$nb))
res<-rbind(res,res2013,natifs,restot)
res$pct<-100*res$nb/restot$nb
kable(res,digits = c(NA,0,2),caption = "Cotonou dans le système migratoire du Bénin (RP 2013)")

```

> Cet exemple montre que près d'un million de personnes recensées au Bénin en 2013 (soit environ 10% de la population) sont "passées" à un moment ou un autre de leur parcours migratoire par Cotonou. On remarque aussi que le nombre de natifs de Cotonou présents au Bénin en 2013 (613450) est supérieur à la population de la commune à cette date (581352). il s'agit donc d'une plaque tournante migratoire majeure pour le pays, mais dont l'espace est saturé et qui redistribue plus de population vers le reste du pays qu'elle n'en reçoit.

### Exemple des mobilités depuis ou vers Parakou

Le programme précédent peut être transformé en fonction applicable à n'importe quelle commune du Benin et ultérieurement implanté sous forme de site web pour apprécier les différences de fonctionnement des territoires. A titre d'exemple, considérons justes le cas de Parakou (code 045), métropole en pleine croissance qui polarise le nord du pays.


```{r}
com<-"045"
cas1<-dt[loc1==com][loc2==com][loc3==com][,cas:=1]
cas2<-dt[loc1!=com][loc2==loc1][loc3==com][,cas:=2]
cas3<-dt[loc1!=com][loc2!=loc1][loc2!=com][loc3==com][,cas:=3]
cas4<-dt[loc1==com][loc2!=com][loc3==com][,cas:=4]
cas5<-dt[loc1==com][loc2==loc1][loc3!=com][,cas:=5]
cas6<-dt[loc1==com][loc2!=loc1][loc3!=com][loc3!=loc2][,cas:=6]
cas7<-dt[loc1!=com][loc2==com][loc3==loc1][,cas:=7]
cas8<-dt[loc1!=com][loc2==com][loc3!=loc1][loc3!=com][,cas:=8]
tabcom<-rbind(cas1,cas2,cas3,cas4,cas5,cas6,cas7,cas8)
tabcom$cas<-as.factor(tabcom$cas)
levels(tabcom$cas) <-c("1. Résidence permanente" ,
                       "2. Arrivée sans relais " ,
                       "3. Arrivée avec relais"  ,
                       "4. Aller et retour"      ,
                       "5. Départ sans relais"   ,
                       "6. Départ avec relais"   ,
                       "7. Passage puis retour"  ,
                       "8. Passage puis départ")

res<-tabcom[,.(nb=.N),.(cas)]
restot<-data.table(cas="Total",nb=dim(tabcom)[1])
res2013<-data.table(cas="Résidents 2013 (1+2+3+4)",nb=sum(res[1:4]$nb))

res[c(1,4,5,6),sum(nb)]
natifs<-data.table(cas="Natifs (1+4+5+6)",nb=sum(res[c(1,4,5,6)]$nb))
res<-rbind(res,res2013,natifs,restot)
res$pct<-100*res$nb/restot$nb
kable(res,digits = c(NA,0,2),caption = "Cotonou dans le système migratoire du Bénin (RP 2013)")

```
> Cet exemple montre qu'environ 300 000 de personnes recensées au Bénin en 2013 sont "passées" à un moment ou un autre de leur parcours migratoire par Parakou. On remarque aussi que le nombre de natifs de Parakou présents au Bénin en 2013 (194345) est supérieur à la population de la commune à cette date (239026). A l'inverse de Cotonou, cette commune de Parakou retient donc plus de migrants qu'elle n'en redistribue vers le reste du pays. Il s'agit d'un pôle d'accumulation. 


### Exemple de modèle de choix conditionnel : Parakou ou Cotonou ?

Pour répondre à la demande de France et Mouftaou de construire des applications des modèles logistique ou multi-niveau, on peut construire un exemple simple de modèle de choix en considérant les personnes chef de ménage et âgées de plus de 18 ans en 2013 qui ont quitté leur commune de naissance pour se rendre respectivement à Cotonou (0) ou à Parakou (1). On trouve 128473 individus répondant à ces critères parmi lesquels on tire au hasard un échantillon de 1000 personnes :


```{r}
sel<-dt[!loc1 %in% c("045","081")][loc3 %in% c("045","081")][dat1<1995][ref=="O",.(sex,age = 2013-dat1,ori=loc1_name,dest=as.character(loc3_name))]
kable(head(sel))
```
#### Effet isolé du sexe

```{r}
tabcont<-table(sel$sex,sel$dest)
test<-chisq.test(tabcont)
chi<-round(test$statistic,2)
pval<-cut(test$p.value,breaks=c(0,0.001,0.01,0.1,1))
levels(pval)<-c("***","**","*","n.s.")
res<-paste("Chi-2 = ", chi, ", df = ",test$parameter,"sign = ", pval)
plot(tabcont,col=c("lightyellow","orange"), main = "Les femmes préfèrent Cotonou ?", sub=res)

```


#### Effet isolé de l'âge


```{r}
sel$age2<-cut(sel$age,breaks=c(18,25,35,45,55,65,100))
tabcont<-table(sel$age2,sel$dest)
test<-chisq.test(tabcont)
chi<-round(test$statistic,2)
pval<-cut(test$p.value,breaks=c(0,0.001,0.01,0.1,1))
levels(pval)<-c("***","**","*","n.s.")
res<-paste("Chi-2 = ", chi, ", df = ",test$parameter,"sign = ", pval)
plot(tabcont,col=c("lightyellow","orange"), main = "Les jeunes préfèrent Parakou ?", sub=res)

```





# DONNEES HUMANITAIRES (HDX)


## La source HDX (Humanitarian Data Exchange) 

Il s'agit d'une source majeure qui semble utilisable dans tous les pays. 


Le sigle **OCHA** désigne le **United Nation Office for the Coordination of Human Affairs** dont la vocation est de fournir une aide dans les situations d'urgence. Sa mission qui est expliquée en détail sur [son site web](https://www.unocha.org/about-ocha) comporte de nombreux volets. Cette agence est organisée sur un plan régional et comporte notamment une délégation spécialisée dans les pays d'Afrique Centrale et de l'Ouest appelée **OCHA-ROWCA** qui couvre l'ensemble despays ciblés par l'école d'été du CIST.



Pour mener à bien ses actions l'OCHA-ROWCA a développé un grand nombre de bases de données qui sont mises à jour régulièrement avec un accès à première vue facile. Ces données sont accessiblesà travers la plateforme [Humanitarian Data Exchange (HDX)](https://data.humdata.org/) qui semble avoir été mise au point par OCHA. Cette plateforme HDX ne comporte pas seulement des données mais aussi différents outils de visualisation ou d'analyse. 

Cela pourrait donc constituer une source majeure de données pour l'école d'été CIST 2022-2023, mais il faut en faire un inventaire précis. L'objet de cette note est de procéder à quelques explorations préliminaires en prenant comme cible le Bénin. 


Si nous effectuons une requête `Benin`  sur le site de données HDX ce qui conduit à une liste de 142 sources de données provenant soit de OCHA-ROWCA, soit d'autres agences des Nations-Unies (FAO, WorldBank, WolrdPop, ...). On trouve encore plus de sources (environ 200) si on utilise le menu de recherche par pays car le moteur ajoute dans ce cas des bases de données transnationales. 

Le HDX est donc un véritable hub de concentration des données les plus récentes sur les pays qui nousintéressent pour l'école d'été, et pas seulement le Benin.



```{r}
knitr::include_graphics("data/figures/HDX.png")
```

Comme on ne peut tout explorer d'un coup, nous commençons par les données qui viennet à proprement parler d'OCHA-ROWCA. 


Dans les sections suivantes nous proposons de rassembler des tableaux de données harmonisées en vue de l'école d'été CIST 2022. Nous privilégions l'exemple du Bénin qui sera le lieu de la formation. Chaque tableau de données fait l'objet d'un exemple rapide d'application de méthodes statistiques ou carographiques afin de vérifier la fiabilité et la qualité des données 

## Limites administratives 


### Métadonnées

Le dossier des limites administratives du Benin fourni par HDX ([cliquer ici](https://data.humdata.org/dataset/benin-administrative-boundaries)) est accessible au format shapefile et comporte différentes couches correspondant aux différents niveaux administratifs. Sa datation laisse augurer qu'il s'agit de la situation la plus récente: les fichiers datent de juin 2021 mais leur nom comporte la séquence "20190816" qui doit plutôt correspondre au 16 Août 2019. En pratique, les limites sont celles du recensement de 2013. 


**N.B.** nous avons toutefois découvert une erreur importante dans le code de la commune d'Adjara qui est noté *BJ0101* alors que le code exact est *BJ1001*. Nous avons également constaté une variabilité des noms de vraiables désignant les codes dans la base OCHA. C'est pourquoi nous avons préféré harmoniser les fichiers. 

```{r, fig.width=5,fig.cap="Départements et communes du Bénin (Source : OCHA-ROWCA)"}

adm0<-st_read("data/admin/ben_adm0.shp",quiet = TRUE)
adm1<-st_read("data/admin/ben_adm1.shp",quiet = TRUE)
adm2<-st_read("data/admin/ben_adm2.dbf", quiet = TRUE)

```


### Exemple d'application

Pour vérifier la qualité des fichiers nous effectuons un calage sur un fonds de carte OpenStreetMap à l'aide du package `leaflet` : 


```{r, fig.width=5,fig.cap="Superposition des fonds de carte sur OSM"}



map <- leaflet() %>% 
            addTiles() %>%
            setView(lat = 8, lng=2.1, zoom = 6) %>%
              addPolygons(data = adm2,
                          label = ~ADM2_NAME,
                          highlightOptions = TRUE,
                           fill = TRUE,
                           color = "orange",
                           weight = 3 ) %>%
             addPolygons(data = adm1,
                        fill = FALSE,
                        color = "red",
                        weight = 2) %>%
             addPolygons(data = adm0,
                        fill = FALSE,
                        color = "black",
                        weight = 1) 
  

map
```



## Pyramide des âges 2019 

### Métadonnées

Un dossier du HDX [accessible ici](https://data.humdata.org/dataset/benin-administrative-level-0-2-sadd-2019-projected-population-statistics) fournit les populations par âge et par sexe en 2019 au niveau des communes, départements ou pays entier. Il s'agit naturellement d'**estimations** mais les données n'en sont pas moins très utile pour toute analyse travaillant sur les situations présentes. Les classes d'âges sont suffisamment détaillées (tranches de 5 ans de 0 à 80 ans) pour procéder à des analyses démographiques intéressantes. 


### Exemple d'application

A titre de vérification, on réalise une analyse factorielle des correspondances AFC sur la pyramide des âges des 77 communes


```{r, fig.cap="AFC sur les structures par âge et sexe des communes du Benin en 2019 (Source : OCHA-ROWCA)"}
don<-read.table("data/pop2019/ben_adm2_pop.csv", sep=";",header=T, encoding = "UTF-8")
don2<-don[,c(11:44)]
row.names(don2)<-don$ADM2_PCODE

afc<-CA(don2,ncp = 10,graph = FALSE)
library(explor)
res <- explor::prepare_results(afc)
explor::CA_var_plot(res, xax = 1, yax = 2, lev_sup = FALSE, var_sup = FALSE,
    var_sup_choice = , var_hide = "Row", var_lab_min_contrib = 0, col_var = "Position",
    symbol_var = NULL, size_var = NULL, size_range = c(10, 300), labels_size = 10,
    point_size = 56, transitions = TRUE, labels_positions = "auto", xlim = c(-0.234,
        0.332), ylim = c(-0.187, 0.38))
```

On enchaîne par une CAH (sur les axes de l'AFC) :

```{r, fig.cap = "Type de structures par âge et sexe des communes du Benin en 2019 (source : OCHA)" }
cah <- HCPC(afc,nb.clust = 5,graph = FALSE)
plot.HCPC(cah,choice = "tree")

```



```{r, fig.cap = "Position des classes de la CAH sur les axes de l'AFC (source : OCHA)"}
plot.HCPC(cah,choice = "map", label.cex=0.3)
```

Il reste à visualiser les classes. On crée pour cela une fonction assez complexe de comparaison entre le profil de celles-ci et le profil du Bénin, inspirée d'un programme trouvé sur un [forum du CIRAD](http://forums.cirad.fr/logiciel-R/viewtopic.php?t=9381) 

```{r, fig.height=8, echo=FALSE, fig.cap = "Profils moyens des classes (source : OCHA)"}
pyrclas <- function(numclas=1) {  
ages<- data.table(cah$data.clust)
fem<-ages[,list(F_00_04 = sum(F_00_04),
                 F_05_09 = sum(F_05_09),
                 F_10_14 = sum(F_10_14),
                 F_15_19 = sum(F_15_19),
                 F_20_24 = sum(F_20_24),
                 F_25_29 = sum(F_25_29),
                 F_30_34 = sum(F_30_34),
                 F_35_39 = sum(F_35_39),
                 F_40_44 = sum(F_40_44),
                 F_45_49 = sum(F_45_49),
                 F_50_54 = sum(F_50_54),
                 F_55_59 = sum(F_55_59),
                 F_60_64 = sum(F_60_64),
                 F_65_69 = sum(F_65_69),
                 F_70_74 = sum(F_70_74),
                 F_75_79 = sum(F_75_79),
                 F_80plus = sum(F_80plus)
                 ),list(clust)]

femtot<-fem[,list(F_00_04 = sum(F_00_04),
                 F_05_09 = sum(F_05_09),
                 F_10_14 = sum(F_10_14),
                 F_15_19 = sum(F_15_19),
                 F_20_24 = sum(F_20_24),
                 F_25_29 = sum(F_25_29),
                 F_30_34 = sum(F_30_34),
                 F_35_39 = sum(F_35_39),
                 F_40_44 = sum(F_40_44),
                 F_45_49 = sum(F_45_49),
                 F_50_54 = sum(F_50_54),
                 F_55_59 = sum(F_55_59),
                 F_60_64 = sum(F_60_64),
                 F_65_69 = sum(F_65_69),
                 F_70_74 = sum(F_70_74),
                 F_75_79 = sum(F_75_79),
                 F_80plus = sum(F_80plus)
                 )]



hom<-ages[,list(M_00_04 = sum(M_00_04),
                 M_05_09 = sum(M_05_09),
                 M_10_14 = sum(M_10_14),
                 M_15_19 = sum(M_15_19),
                 M_20_24 = sum(M_20_24),
                 M_25_29 = sum(M_25_29),
                 M_30_34 = sum(M_30_34),
                 M_35_39 = sum(M_35_39),
                 M_40_44 = sum(M_40_44),
                 M_45_49 = sum(M_45_49),
                 M_50_54 = sum(M_50_54),
                 M_55_59 = sum(M_55_59),
                 M_60_64 = sum(M_60_64),
                 M_65_69 = sum(M_65_69),
                 M_70_74 = sum(M_70_74),
                 M_75_79 = sum(M_75_79),
                 M_80plus = sum(M_80plus)
                 ),list(clust)]

homtot<-hom[,list(M_00_04 = sum(M_00_04),
                 M_05_09 = sum(M_05_09),
                 M_10_14 = sum(M_10_14),
                 M_15_19 = sum(M_15_19),
                 M_20_24 = sum(M_20_24),
                 M_25_29 = sum(M_25_29),
                 M_30_34 = sum(M_30_34),
                 M_35_39 = sum(M_35_39),
                 M_40_44 = sum(M_40_44),
                 M_45_49 = sum(M_45_49),
                 M_50_54 = sum(M_50_54),
                 M_55_59 = sum(M_55_59),
                 M_60_64 = sum(M_60_64),
                 M_65_69 = sum(M_65_69),
                 M_70_74 = sum(M_70_74),
                 M_75_79 = sum(M_75_79),
                 M_80plus = sum(M_80plus)
                 )]



# Creation de la base "donnee1"
donnee1<-c(1:17)
donnee1<-as.data.frame(donnee1)
donnee1$age<-c("0-4","5-9","10-14","15-19","20-24","25-29","30-34","35-39",
               "40-44","45-49","50-54","55-59","60-64","65-69","70-74","75-79","80 et +")
donnee1$nbrM<-as.numeric(hom[numclas,2:18])
donnee1$nbrM<-100*donnee1$nbrM/sum(donnee1$nbrM)
donnee1$nbrF<-as.numeric(fem[numclas,2:18])
donnee1$nbrF<-100*donnee1$nbrF/sum(donnee1$nbrF)
Classe = donnee1

donnee2<-c(1:17)
donnee2<-as.data.frame(donnee2)
donnee2$age<-c("0-4","5-9","10-14","15-19","20-24","25-29","30-34","35-39",
               "40-44","45-49","50-54","55-59","60-64","65-69","70-74","75-79","80 et +")
donnee2$nbrM<-as.numeric(homtot)
donnee2$nbrM<-100*donnee2$nbrM/sum(donnee2$nbrM)
donnee2$nbrF<-as.numeric(femtot)
donnee2$nbrF<-100*donnee2$nbrF/sum(donnee2$nbrF)
Benin = donnee2

# Reformat data
donnee <- bind_rows(
  Classe %>% mutate(data = "Classe", Classe = NULL),
  Benin %>% mutate(data = "Benin", Benin = NULL)
) %>%
  mutate(age = factor(x = age, levels = unique(age))) %>%
  gather(key = "label", value = "nombre", nbrM, nbrF) %>%
  mutate(
    sex = factor(x = unname(c("nbrM" = "Hommes", "nbrF" = "Femmes")[label]), levels = c("Femmes", "Hommes")),
    direction = c("nbrM" = -1, "nbrF" = 1)[label],
    alpha = data
  )

g <- ggplot() +
  theme_classic() +
  geom_bar(
    data = filter(donnee, data=="Benin"),
    aes(x = age, y = nombre*direction, fill = sex, group = data, alpha = alpha),
    stat = "identity",
    width = 0.90
  ) +
  geom_bar(
    data = filter(donnee, data=="Classe"),
    aes(x = age, y = nombre*direction, fill = sex, group = data, alpha = alpha),
    stat = "identity",
    width = 0.45
  ) +
  geom_hline(yintercept = 0) +
  coord_flip() +
  scale_y_continuous(labels = function(x){paste0(abs(x), "%")}) +
  scale_alpha_manual(values = c(0.2, 1), breaks = c("Benin", "Classe")) +
  scale_fill_discrete(drop = FALSE) +
  labs(x = "Age", y = NULL, fill = "Sexe")

return(g)

}

leg <- get_legend(pyrclas(1))
p1<-pyrclas(1)+ theme(legend.position = "none")
p2<-pyrclas(2)+ theme(legend.position = "none")
p3<-pyrclas(3)+ theme(legend.position = "none")
p4<-pyrclas(4)+ theme(legend.position = "none")
p5<-pyrclas(5)+ theme(legend.position = "none")



plot_grid(p1, p2, p3, p4, p5, leg,labels = c("1", "2", "3", "4", "5",""), align = "hv",ncol=2)

```

- la **classe 1** : correspond à une population beaucoup plus jeune que la moyenne du pays avec un excédent de 0-9 ans et un déficit de 30 ans et plus. C'est la partie du pays où la transition démographique semble être la plus tardive
- la **classe 2** : présente également un profil de population jeune avec un excédent de 5-14 ans, mais se caractérise par une réduction relative des 0-4 ans qui témoigne soit d'une baisse de la fécondité, soit d'un exode rural des jeunes ménages avec enfants.
- la **classe 3** correspond au profil moyen du pays.
- la **classe 4** est caractérisée par un excédent de personnes âgées et un déficit à la fois de jeunes enfants et de jeunes adultes. Elle correspond à des zones de vieillissment relatif de la population, soit sous l'effet de l'exode rural, soit en raison d'une baisse de la fécondité.
- la **classe 5** est caractérise par une forte surreprésentation des jeunes adultes et des persones âgées, associée à un net déficit des enfants jeunes de 0 à 14 ans. Il s'agit vraisemblablement de zones urbaines caractérisées par une fécodité plus tardive et une meilleure espérance de vie.  

On peut visualiser la distribution de ces cinq classes sur une carte : 


```{r, echo=FALSE, fig.cap = "Distribution spatiale des classes (source : OCHA)"}

par(mfrow = c(1,1))


### Carte classique

adm2$clust<-cah$data.clust$clust

# set theme
mf_theme("candy")
# plot administrative status
mf_map(
  x = adm2, 
  var = "clust", 
  type = "typo",
  pal = c("red", "orange","yellow","green","blue"), 
  lwd = .5,
  leg_pos = "topright",
  leg_title = ""
)

# layout
mf_layout(title = "Carte géométrique",  
          scale = "FALSE",
          credits = paste0("Sources: OCHA, 2019\n",
                            "Ecole d'été CIST 2022"))




```




## Alphabetisation et langues (2013)


### Métadonnées 

Ce fichier qui est accessible sur HDX en suivant [ce lien](https://data.humdata.org/dataset/benin-languages) a été mis à disposition par l'organisation [Translators without borders](https://translatorswithoutborders.org/) mais l'analyse des métadonnées montre qu'il s'agit en fait de données extraites du recensement géénral de population du Bénin de 2013 :


> Created by Translators without Borders,  Uploaded on Jun-21, Version 1

>**Notes and caveats**

>  + All data is drawn from government survey results and is subject to any associated limitations or distortions present in the source data.
>  + Literacy was measured as the ability to read and write in any language.
Languages with population shares less than 0.01% or under 1,000 people (whichever is greater) have been aggregated into the “Other” field.
>  + All decimal values have been rounded to a maximum of 3 decimal places. As a result language shares may not total 100%.
>  + Empty values represent non-existent data and should not be treated as zero values.
Data is available under an Attribution NonCommercial ShareAlike 4.0 International license (CC BY NC SA 4.0)

>**Copyright and terms of use**

>  + You are free to share and adapt the data subject to requirements for attribution and non commercial use. 
>  + Any derivative work must be distributed under the same license as the original. Full terms at 
https://creativecommons.org/licenses/by nc sa/4.0/


Les fichiers fournies comportaient de petites erreurs ou des difficultés de formatage rendant compliquée leur importation dans R. Il a donc fallu les corriger un peu avant de les utiliser. Une fois cela effectué, on a vérifié que les données pouvaient se cartographie en effectuant une jointure avec le fichier administratif au niveau communal.  

**N.B.** nous avons retrouvé l'erreur  dans le code de la commune d'Adjara qui est noté *BJ0101* alors que le code exact est *BJ1001*. Nous avons donc reconstruit les fichiers en les harmonisant avec les fonds de carte. 




### Exemple d'application

Le fichier comporte un ensemble de variables concernant le niveau d'alphabétisation des hommes et des femmes de plus de 6 ans en 2013. Voici à titre indicatif les valeurs par département et pour l'ensemble du pays :  


```{r}
dep<-read.table("data/langues/ben_lang_adm1_correct.csv", sep=";",header =TRUE, dec=",")
dep<-dep[,c(1:2,77:82)]
kable(dep)
```

Ce tableau se prêtera particulièrement bien à des exercices d'initiation à la statistiques et la cartographie sous R puisqu'il comporte à la fois des stocks (population par sexe), des taux (% de population alphabétisée par sexe) et une possibilité de les combiner pour retrouver le nombre de personnes alphabétisée. Il s'inscrit par ailleurs pleinement dans la thématique des inégalités territoriales. 

On peut à titre d'exemple construire une cartographie du niveau d'alphabétisation des hommes et des femmes par département du Bénin en 2013 (carte choroplèthe) et superposer par dessus les effectifs correspondants d'hommes et de femmes alphabétisés. 



```{r, echo=FALSE, fig.cap = "Alphabétisation des hommes et femmes du Bénin par département en 2013 (source : RP Bénin 2013, via OCHA)"}

# calcul du nombre de pers. alphabétisées par sexe
dep$literacy_female_pop <- dep$literacy_female*dep$pop_female
dep$literacy_male_pop <- dep$literacy_male*dep$pop_male

# Chargement de la géométrie
map <- st_read("data/admin/ben_adm1.shp", quiet=TRUE)  %>% select(ADM1_CODE)     

# Jointure
mapdep<-merge(map,dep,by="ADM1_CODE")

par(mfrow = c(1,2))


mf_theme("darkula")
mapdep %>% 
  mf_map("literacy_female", 
         type = "choro",
          breaks = c(0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9),
         leg_title = "%", 
         leg_pos = "right",
         ) %>%
  mf_map("literacy_female_pop", 
         type = "prop",
         leg_title = "effectif", 
         leg_pos = "left",
         inches=0.1
         ) 

   mf_layout(title = "Femmes", 
          credits = "EE Cist 2022, Source : RP 2013",
          scale = FALSE,
          arrow = FALSE)

mf_theme("darkula")
mapdep %>% 
  mf_map("literacy_male", 
         breaks = c(0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9),
         type = "choro",
         leg_title = "%", 
         leg_pos = "right"
         ) %>%
  mf_map("literacy_male_pop", 
         type = "prop",
         leg_title = "effectif", 
         leg_pos = "left",
         inches=0.1
         ) 
   mf_layout(title = "Hommes", 
          credits = "EE Cist 2022, Source : RP 2013",
          scale = FALSE,
          arrow = FALSE)


```

On voit apparaître alors une assez forte corrrélation entre les deux distributions, même si les femmes sont moins alphabétisés que les hommes. Ce qui suggère un exercice d'apprentissage de la régression linéaire entre les deux variables.

```{r}
mod.reg <- lm(dep$literacy_female~dep$literacy_male)
#stargazer(mod.reg,type = "html")
ggplot(data = dep) + aes(x=100*literacy_male, y = 100*literacy_female, label = ADM1_NAME) +
                     geom_point(col="red") + 
                     geom_smooth(method="lm") +
                     geom_text(cex=3, nudge_y=2) +
                     scale_x_continuous("Taux d'alphabétisation des hommes (%)") +
                     scale_y_continuous("Taux d'alphabétisation des femmes (%)") +
                     ggtitle("Inégalités d'alphabétisation des départements du Benin par sexe",
                             subtitle = "Source : RP 2013 via OCHA")
```



Les résultats mettent en évidence le fait que l'alphabétisation des femmes au Bénin en 2013 est en moyenne inférieure de 20% à celle des hommes (le paramètre b de la régression est non significatif) et la relation est statistiquement très significative (r2  = 93 \%, p < 0.001). Il existe notamment des résidus c'est-à-dire des départements où la scolarisation des femmes estimée en fonction de celle des hommes est plus forte que prévue (*Alibori, Borgou, Littoral*) ou plus faible que prévue (*Plateau, Couffo, Mono*). 

Le même modèle appliqué au niveau des communes permettrait une analyse plus fine des résultats à travers une carte des résidus. On pourrait aussi imaginer une analyse mutiscalaire croisant les écarts des communes à la moyenne du pays, de leur département et des communes voisines à l'aide du package `mta`. Bref, il s'agit sans nul doute d'un excellent exemple pédagogique pour l'école d'été CIST2022.


## Prix de la nourriture sur les marchés (2002-2021)

### Métadonnées

Ce fichier qui est accessible sur HDX en suivant [ce lien](https://data.humdata.org/dataset/wfp-food-prices-for-benin) a été mis à disposition par  [World Food Programme](https://www.wfp.org/). Cette base qui semble avoir été interrompue présente l'avantage d'offrir une grande profondeur historique et une résolution spatialetrès précise puisque les prix sont localisés par marché et par semaine.

>This no longer updated dataset contains Global Food Prices data from the World Food Programme covering foods such as maize, rice, beans, fish, and sugar for 76 countries and some 1,500 markets. It is updated weekly but contains to a large extent monthly data. The data goes back as far as 1992 for a few countries, although many countries started reporting from 2003 or thereafter.


Le fichier contenant les coordonnées de latitude et de longitude des marchés, il peut théoriquement être cartographié facilement à l'aide d'un outil de cartographie dynamique tel que `mapview`. Mais il peut aussi faire l'objet de multiples agrégations spatiales, temporelles, par produit. Reste évidemment à en apprécier la qualité ...

Noter qu'il est nécessaire d'**éliminer la seconde ligne du tableau initial** sous excel pour pouvoir ensuite charger correctement le fichier. Il est également préférable de lire le fichier avec la fonction *fread()* du package `data.table` faut de quoi on peut avoir des problèmes d'encodage plus ou moins délicats à résoudre. 


```{r}
dt<-fread("data/foodprices/wfp_food_prices_ben.csv")
kable(head(dt))
```

On peut assez facilement spatialiser le fichier qui comporte en tout 51 marchés assez bien répartiis sur l'ensemble du pays comme le montre la carte ci-desous :


```{r}
loc<-dt %>% filter(duplicated(market)==FALSE) %>% select(c(2,3,4,5,6)) %>% arrange (admin1,admin2, market)
markets <- st_as_sf(loc, coords = c("longitude","latitude"))
st_crs(markets)<- 4326
#mapview(markets)


adm0<-st_read("data/admin/ben_adm0.shp",quiet = TRUE)
adm1<-st_read("data/admin/ben_adm1.shp",quiet = TRUE)
adm2<-st_read("data/admin/ben_adm2.shp", quiet = TRUE)

par(mar=c(0,0,0,0))
plot(adm2$geometry,col="gray90", border = "gray20",lwd=0.5)
plot(adm1$geometry,col=NA, border = "black",lwd=1, add=TRUE)
plot(adm0$geometry,col=NA, border = "black",lwd=2, add=TRUE)
plot(markets$geometry,col="red", bg="yellow",pch=21,add=TRUE,cex=0.7)
```

### Exemple d'application

Cette base de données **multidimensionelle** peut servir d'exemple pour de nombreux exercices d'apprentissage de la statistique et de la cartographie sous R. Le prix des produits alimnentaires dépend en effet :

- du type de produit (QUOI) ?
- de la date d'achat (QUAND) ?
- du lieu d'achat (OU ?)

Pour chacune des dimensions, on peut réaliser des procédures **d'agrégation** des valeurs observées ou **d'estimation** des valeurs manquantes. A titre d'exemple, on extrait de la base de donnée l'ensemble des lignes qui concernent le riz

```{r}
riz<- dt %>% filter(substr(commodity,1,4)=="Rice") %>% 
               select(when = date, 
                      where1 = admin1,
                      where2 = admin2,
                      where3 = market,
                      what = commodity,
                      cost = usdprice )
kable(head(riz))
```


On peut calculer le prix median du riz sur l'ensemble des points de sondage disponibles en fonction de la catégorie de riz : 

```{r}
tab<- riz %>% group_by(when, what) %>% summarize(median_cost=median(cost), nbobs = n())
ggplot(tab) + aes(x=when,y=median_cost, color=what, size=nbobs) + geom_point() + 
  scale_x_continuous("Données de prix mensuelles") + 
  scale_y_continuous("Prix médian d'un kilog de riz (en $)")+
  ggtitle(label = "Estimation des prix du riz au Bénin",subtitle = "Source : World Food Program (via HDX)")
```

Le graphique montre plusieurs choses :

- les données les plus complètes concernent le riz importé qui augmente fortement au moment de la crise de 2007-2008
- la spatialisation des données n'est pas vraiment possible avant 2015 voire 2018
- il y a de fortes différences de prix selon le type de riz


Effectuons maintenant un zoom sur le priz médian du riz importé en 2020-2021 par régions :

```{r}
tab <- riz %>% filter(when >=as.Date("2020-04-01"), what =="Rice (imported)") %>%
               group_by(when,where1) %>%
               summarize(median_cost=median(cost))
ggplot(tab) + aes(x=when,y=where1, fill=median_cost) +geom_tile() + 
  scale_fill_gradient(low="white", high = "red")+
    scale_x_date("Médiane des valeurs observées par mois") + 
  scale_y_discrete("Département")+
  ggtitle(label = "Prix du riz importé sur les marchés du Benin ($/kg)",subtitle = "Source : World Food Program (via HDX)")
                        


```









## Villages et localités vers 2015


### Métadonnées 

Ce fichier accessible en [cliquant ici](https://data.humdata.org/dataset/benin-settlements) concerne le peuplement, c'est-à-dire apparemment l'inventaire de toutes les localisations avec leur nom et leur position en latitude longitude. Il comporte 6306 entrées. Les fichiers sont datés de 2015. 

```{r, fig.width=5,fig.cap="Village et peuplement au Benin vers 2015 (Source : OCHA-ROWCA)"}
loc<-st_read("data/settlement/ben_plp_NGA.shp", quiet = TRUE)
par(mar=c(0,0,0,0))
plot(adm2$geometry,col="lightyellow", border = "gray20",lwd=0.5)
plot(loc$geometry,col="red",pch=16,cex=0.2, add=T)

```


### Exemple d'application

A titre de vérification de la précision, nous effectuons une superposition sur le fonds de carte OpenStreetMap pour la commune de Ouidah (code BJ0304 ou BEN003004) ou aura lieu l'école d'été du CIST. En cliquant sur les points onpeut comparer leur nom avec celui des localités indiquées par OSM.

```{r, fig.width=5,fig.cap="Projection des données OCHA-ROWCA sur Open Street Map (commune de Ouidah"}
map<-adm2[adm2$admin2Pcod=="BJ0304",]
map<-st_transform(map,4326)

vil<-loc[loc$RowcaCode2=="BEN003004",]
vil<-st_transform(vil,4326)


map <- leaflet() %>% 
            addTiles() %>%
            setView(lat = 6.4, lng=2.1, zoom = 11) %>%
            addPolygons(data = map,
                        fill = FALSE,
                        color = "red",
                        weight = 2) %>%
             addMarkers(data = vil,
                        label = ~FULL_NAME_)

map
```

Il y a à l'évidence des décalages ... Et le contour de la commune lui-même ne semble pas coller exactement avec celui fourni par OSM. Il faudra vérifier laquelle des deux sources est erronée (à moins que ce ne soient les deux ?). Toutefois, la localisation semble rester approximativement juste ...


# DONNEES GLOBALES (Banque Mondiale)

Nous allons présenter ici deux packages R qui correspondent à des API permettant de télécharger respectivement des données statistiques de la Banque Mondiale et des fonds de carte Natural Earth. Chacune de ces API a été implémentée sous le forme de package R ce qui permet d'extraire facilement les données, dès lors qu'on dispose d'une connexion internet suffisante. Un intérêt évident de cette approche par API est de pouvoir effectuer facilement des requêtes sur n'importe quelle partie du monde et de pouvoir mettra à jour régulièrement les données au fur et à mesure de leur mise à jour. 

A titre d'exemple, nous allons montrer comment réaliser une carte des émissions de CO2 par habitant des pays de la CEDEAO en 2018. 


## L'API Banque Mondiale 

Supposons que l'on souhaite télécharger la population, le PIB et les émisssions de CO2 des pays du monde de 2000 à 2015. Plutôt que d'aller chercher des fichiers sur un site web, nous allons utiliser une API proposée par la Banque Mondiale qui permet de télécharger les données facilement et surtout de les mettre à jour régulièrement. Pour cela on va installer le package R correspondant à l'API `wbstats` de la Banque mondiale.

https://cran.r-project.org/web/packages/wbstats/vignettes/Using_the_wbstats_package.html



Au moment du chargement du package, il est créé un fichier wb_cachelist qui fournit l'ensemble des donnes disponibles sous la forme d'une liste de tableaux de méta-données.




```{r}
library("wbstats")
cat<-wb_cachelist
str(cat,max.level = 1)
```


### Le tableau "countries"

Il fournit des renseignements de base sur les différents pays, leurs codes, etc.

```{r}
str(cat$countries)
```

Le tableau comporte 304 observation et il mélange des pays (France), des fragments de pays (Réunion) et des agrégats de pays (Europe). Il faudra donc bien faire attention lors de l'extraction à réfléchir à ce que l'on souhaite utiliser. Par exemple, si l'on veut juste les pays :

```{r}
## Programme en langage R_base
# pays<-cat$countries[cat$countries$income_level!="Aggregates",c("iso3c", "country","capital_city","longitude","latitude", "region","income_level")]


## Programme en langage dplyr

pays <- cat$countries %>% 
          filter(income_level !="Aggregates") %>%
          select(iso3c,country, capital_city, latitude, longitude, region, income_level)


kable(head(pays))

```

### Le tableau indicators

Il comporte pas loin de 17000 variables ... Autant dire qu'il est difficile de l'explorer facilement si l'on ne sait pas ce que l'on cherche. 

```{r}
indic<-cat$indicators
dim(indic)
kable(head(indic))
```
#### Recherche du code d'un indicateur

Supposons qu'on recherche les données récentes sur les émissions de CO2. On va utiliser le mot-clé *CO2* pour rechercher les variables correspondantes dans le catalogue à l'aide de la fonction `wbsearch`, ce qui donne 45 réponses 

```{r}
vars <- wbsearch(pattern = "CO2",fields="indicator")
kable(head(vars))
```

On va finalement trouver le code de la variable recherchée

- *EN.ATM.CO2E.KT* : émissions de CO2 en kilotonnes

Les deux autres variables dont nous avons besoin ont pour code 

- *NY.GDP.MKTP.CD* : PIB en parités de pouvoir d'achat
- *SP.POP.TOTL* : Population totale


#### Extraction des métadonnées 

Une fois que l'on pense connaître le code de nos variables, on peut extraire les métadonnés pour vérifier qu'il s'agit bien de ce que l'on cherche, quelle est la source exacte, quelle est l'unité de mesure ...

```{r}
# Programme R-base
meta<-cat$indicators[cat$indicators$indicator_id %in% c("SP.POP.TOTL","NY.GDP.MKTP.CD","EN.ATM.CO2E.KT"),]

# Programme dplyr
meta<-cat$indicators %>%
        filter(indicator_id %in% c("SP.POP.TOTL","NY.GDP.MKTP.CD","EN.ATM.CO2E.KT"))

kable(meta)
```


### L'extraction des données

Elle se fait à l'aide de la fonction `wb_data` qui comporte de nombreuses options. 



#### le paramètre `indicator = `

Ce paramètre permet de choisir les indicateurs à collecter, ce qui suppose que l'on connaisse leur code. Par exemple, supposons que l'on veuille extraire la population et le PIB pour pouvoir calculer ensuite le PIB par habitant

```{r, eval=FALSE}
df   <- wb_data(indicator  = c("NY.GDP.MKTP.CD","SP.POP.TOTL", "EN.ATM.CO2E.KT"))


```

- **commentaire** : Nous obtenons un tableau très grand (> 13000 lignes) qui comporte les valeurs pour toutes les dates disponibles depuis 1960 et pour tous les pays, même si les valeurs sont souvent manquantes. 


### le choix d'une période de temps

#### les paramètres `startdate = ` et `startdate = ` 

Ces deux paramètres permettent de choisir une plage de temps. On peut par exemple décider de ne collecter que les données relatives aux années 2017, 2018 et 2019 

```{r}
df   <- wb_data(indicator  = c("NY.GDP.MKTP.CD","SP.POP.TOTL", "EN.ATM.CO2E.KT"),
                start_date = 2017,
                end_date = 2019)
dim(df)
kable(head(df,6))

```

- **commentaire** : Le tableau ne comporte donc plus que 651 lignes correspondant aux trois dates pour les différents pays du Monde. 


#### Le paramètre `mrv` (most recent value)

Lorsque l'on souhaite juste obtenir les données les plus récentes, on peut remplacer les paramètres `startdate = ` et `startdate = `  par le paramètre `mrv = ` suivit d'un chiffre indiquant le nombre d'années que l'on souhaite à partir de la date la plus récente. Avec mrv=1 on récupère uniquement la dernière année disponible pour au moins l'une des variables.

```{r}
df   <- wb_data(indicator  = c("NY.GDP.MKTP.CD","SP.POP.TOTL","EN.ATM.CO2E.KT"),
                mrv = 1)
dim(df)
kable(head(df,6))
```

L'inconvénient de cette méthode est que cela peut aboutir à un grand nombre de valeurs manquantes si l'une des variables recherchée n'a pas été mise à jour. 
Il est donc préférable de sélectioner une période plus longue mrv=5 et de faire ensuite soi-même le tri

### Le choix des unités géographiques



Le paramètre `country = ` permet de choisir les entités spatiales à collecter, soit sous forme de liste de codes, soit à l'aide de valeurs spéciales. Par défaut; il renvoie la liste de tous les pays, mais on peut se limiter à quelques uns seulement à l'aide de leur nom en anglais (risqué ...) ou de leur code ISO3 (plus sûr)

#### sélection de pays

```{r}
df   <- wb_data(indicator  = c("NY.GDP.MKTP.CD","SP.POP.TOTL","EN.ATM.CO2E.KT"),
                start_date = 2018,
                end_date = 2018,
                country = c("BEN","TGO"))
df$GDP.per.capita <- round(df$NY.GDP.MKTP.CD / df$SP.POP.TOTL,0)
df$CO2.per.capita <- round(1000*df$EN.ATM.CO2E.KT / df$SP.POP.TOTL,2)
kable(head(df,6))
```

- **commentaire** : Il est donc facile de travailler sur un petit nombre de pays que l'on souhaite comparer.

#### Opérateurs spéciaux

Il existe un certain nombre de paramètres spéciaux que l'on peut utiliser à la place de la liste des pays :

- "countries_only" (Default)
- "regions_only"
- "admin_regions_only"
- "income_levels_only"
- "aggregates_only"
- "all"




```{r}
df   <- wb_data(indicator  = c("NY.GDP.MKTP.CD","SP.POP.TOTL","EN.ATM.CO2E.KT"),
                start_date = 2018,
                end_date = 2018,
                country = "regions_only")
df$GDP.per.capita <- round(df$NY.GDP.MKTP.CD / df$SP.POP.TOTL,0)
df$CO2.per.capita <- round(1000*df$EN.ATM.CO2E.KT / df$SP.POP.TOTL,2)
kable(df)
```

- **commentaire** : Nous avons extrait les données par grandes régions du Monde pour l'année 2018

### Le format de sortie du tableau

Il existe deux façons d'extraire un tableau comprenant plusieurs variables ou plusieurs dates, selon que l'on veut un tableau large (wide) ou étroit. On peut régler la sortie à l'aide du paramètre `return_wide` qui est TRUE par défaut mais que l'on peut régler sur FALSE.

#### `return_wide` = FALSE

```{r}
df   <- wb_data(indicator  = c("NY.GDP.MKTP.CD","SP.POP.TOTL","EN.ATM.CO2E.KT"),
                return_wide = TRUE,
                start_date = 2017,
                end_date = 2018,
                country = c("BEN","TGO"))
df
```

#### `return_wide` = FALSE

```{r}
df   <- wb_data(indicator  = c("NY.GDP.MKTP.CD","SP.POP.TOTL","EN.ATM.CO2E.KT"),
                return_wide = FALSE,
                start_date = 2017,
                end_date = 2018,
                country = c("BEN","TGO"))
df[,1:7]
```


## L'API Natural Earth


###  Natural Earth

Nous allons ici utiliser le fonds de carte *Natural Earth* qui est un fonds de carte libre de droit et mis à jour régulièrement. Le site web du projet se situe à l'adresse suivante :

https://www.naturalearthdata.com/

Il indique ses objectifs comme suit : 

> "Natural Earth is a public domain map dataset available at 1:10m, 1:50m, and 1:110 million scales. Featuring tightly integrated vector and raster data, with Natural Earth you can make a variety of visually pleasing, well-crafted maps with cartography or GIS software.[...] Natural Earth was built through a collaboration of many volunteers and is supported by NACIS (North American Cartographic Information Society), and is free for use in any type of project (see our Terms of Use page for more information)."

On peut télécharger les différents fonds de carte sur le site web, mais dans une perspective de mise à jour automatique régulière du fonds de carte il est plus pertinent d'utiliser l'API `rnaturalearth`qui permet d'accéder directement à la plupart des fonds de carte avec juste quelques lignes de code. Il suffit pour cela de commencer par installer et charger le package.

```{r}
library("rnaturalearth")
library("rnaturalearthdata")
```



### le fonds de carte *countries110* (175 unités)

On va télécharger tout d'abord le fonds de carte des pays du Monde avec une forte généralisation des contours `countries110` et le transformer en objet de type spatial feature du package  `sf` du package avant de le visualiser et d' examiner le nombre d'unités 


```{r}
map<-st_as_sf(countries110)
class(map)

ggplot(data = map) +
        geom_sf(fill="lightyellow") +
        theme_bw()


```

Ce fonds de carte comporte 175 unités spatiales, mais de quoi s'agit-il exactement. Les métadonnées associées permettent de se faire une idée plus précise de la nature exacte de ces unités. Prenons pour cela quelques exempes

```{r}
sel<-map[map$adm0_a3 %in% c("FRA", "NCL","ATA","ATF","USA", "PRI","CHN","TWN","MAR", "SAH","CHN","TWN","ISR","PSX"),c("sovereignt","sov_a3","type","admin", "adm0_a3","name","note_adm0","iso_a3","wb_a3")]
kable(sel)
```

Les exemples présentés dans le tableau ci-dessus montrent la complexité du problème de définition et de représentation cartographique des "pays" ou "bouts du monde". Quelques remarques :

1. La *France* (FR1) en tant qu'état souverain regroupe ici cartographiquement la partie métropolitaine du pays et les Départements d'Outre-Mer (Guyane Française, Réunion, Martinique, Guadeloupe) en une seule entité spatiale, mais elle met à part la Nouvelle Calédonie et les îles antarctiques. 
2. *Porto Rico* (PRI) est considéré comme une dépendance des *Etats-Unis* (US1) au même titre que la *Nouvelle Calédonie*(NCL) est considérée comme une dépendance de la *France* (FR1).
3. Le *Sahara occidental* (SAH) est considéré comme une zone *indéterminée* bien qu'il soit occupé par le *Maroc* (MAR).
4. la Palestine (PSX) est considéré comme une zone *disputée* mais rattachée en terme de souveraineté à *Israël* (ISR) et une note précise qu'elle est *partiellement semi-administrée*. Le code sur trois caractères des territoires palestiniens est très variable selon les organisations (PSX, PSE, WBG).
5. *Taïwan* (TWN) est présenté comme un état souverain, mais son code ISO3 est manquant pour la banque mondiale car la Chine refuse de le reconnaître. 
6. Plusieurs états souverains de petite taille sont absents de ce fonds de carte qui ne regroupe que 175 unités soit moins que les 193 pays membres des Nations-Unies. La plupart des îles du Pacifique sont en particulier éliminées car leur surface les rendrait invisible pour le degré de généralisation cartographique adopté. 


###  le fonds de carte *sovereignty110* (171 unités)

On peut obtenir un fonds différent en installant le package complémentaire `rnaturalearthdata` qui permet notamment de distinguer le fonds de carte des *countries* (c'est-à-dire des "bouts du monde" souverains ou non) et des *sovereignty* (c'est-à-dire des états souverains)

```{r}
library(rnaturalearthdata)
map<-st_as_sf(sovereignty110)

ggplot(data = map) +
 geom_sf(fill="lightyellow") +  
  theme_bw()

```

Le fonds de carte permet désormais de récupérer la plupart des pays souverains du Monde, y compris les petits états insulaires du Pacifique, mais il fait disparaître de façon sélective les territoires indéterminés ou disputés. Ainsi, le Sahra Occidental demeure partiellement séparé du Maroc mais les territoires palestiniens sont annexés à Israël ainsi que le plateau du Golan ce qui n'est évidemment pas un choix neutred'un point de vue géoolitique.

```{r}
par(mfrow=c(1,2))

map2<-map %>% filter(sov_a3 %in% c("ISR","JOR","SYR","LBN","EGY"))
ggplot(data = map2) +
    geom_sf(fill=c("gray80","orange","gray80","gray80","gray80")) +
    ggtitle("Limits of Israël") +
  theme_minimal()


map3<-map %>% filter(sov_a3 %in%c("MAR","SAH","DZA","MRT"))
ggplot(data = map3) +
    geom_sf(fill=c("gray70","orange","gray70","lightyellow")) +
    ggtitle("Limits of Morocco") +
  theme_minimal()




```



### Le fonds de carte *countries50* 

On peut également choisir un fonds moins généralisé dans lequel tous les petits pays seront présents

```{r}

map<-st_as_sf(countries50)
ggplot(data = map) +
    geom_sf(fill="lightyellow") +
  theme_bw()

```

Il existe toute une série d'autres fonds de carte dans le package Natural Earth, notamment avec des résolutions plus précises, mais on se limitera ici à l'exploration des fonds de carte utile pour produire des cartes à contour généralisé couvrant le monde entier.


## Exemple d'application

Nous allons essayer de constituer une carte des émissions de CO2 par habitant des pays de la CEDEAO en 2018 basée sur la combinaison des données `wbstats` et du fonds de carte `naturalearth`.

### Etape 1 : récupération des données statistiques


Nous commençons par récupérer les données brutes de population et de CO2 en 2018 pour les pays de la CEDEAO et on y ajoute les latitues et longitues des centres des pays.

```{r}
cedeao<-c("BEN","BFA","CPV","CIV", "GMB","GIN","GNB","GHA","LBR","MLI","NER","NGA","SEN","SLE","TGO")
df   <- wb_data(indicator  = c("SP.POP.TOTL", "EN.ATM.CO2E.KT"),
                return_wide = TRUE,
                start_date = 2018,
                end_date = 2018,
                country = cedeao)
kable(df)

```

Nous renommons les variables pour avoir un tableau plus simple ou la population est en millions d'habitants, les émissions de CO2 en millions de tonnes. On y ajoute l'intensité des émissions en tonnes par habitant.



```{r}

don <-df %>% select(ISO3 = iso3c, NOM = country, POP = SP.POP.TOTL, CO2 = EN.ATM.CO2E.KT) %>%
            mutate(POP = POP/1000000, CO2 = CO2/1000, CO2_hab = CO2/POP)
kable(don,digits = 2)

```





### Etape 2 : Récupération du fonds de carte

on récupère ensuite le fonds de carte en ne gardant que les pays de la CEDEAO

```{r}
cedeao<-c("BEN","BFA","CPV","CIV", "GMB","GIN","GNB","GHA","LBR","MLI","NER","NGA","SEN","SLE","TGO")
map<-st_as_sf(countries110)
map<-st_as_sf(countries110) %>% 
        select(adm0_a3,geometry) %>% 
        rename(ISO3 = adm0_a3) %>%
        filter(ISO3 %in% cedeao)
plot(map$geometry, col="lightyellow")
```



### Etape 3 : Jointure du fonds de carte et des statistiques

```{r}
mapdon <- right_join(don,map) %>% st_as_sf()
kable(head(mapdon))
```

### Etape 4 : Visualisation avec mapsf


```{r}

mf_theme("agolalight")
mapdon %>% 
  mf_map("CO2_hab", 
         "choro",
         breaks="jenks",
         leg_pos="bottomleft",
         leg_title = "en tonnes/hab.")%>%
  mf_map("CO2", 
         "prop",
         col="red",
         leg_pos = "topleft",
         leg_title = "en millions de tonnes")
mf_title("Emissions de CO2 des pays de la CEDEAO en 2018")

```




# AUTRES DONNEES

Sous le terme de données *non conventionnelles* on peut regrouper toute une série de bases de données qui ne sont pas issues de la statistique publique mais qui peuvent faire l'objet d'exploitations intéressantes. Cela recouvre notamment les données issues des réseaux sociaux (facebook, twitter, ...) mais aussi les données collaboratives (Open Street Map) où les données d'entreprise privée mises à disposition du public. 

## Données médiatiques

A titre d'exemple de données non conventionnelles, nous proposons d'examiner le cas des flux RSS de deux journaux du Benin, mis à disposition par le site de recherche américain Media Cloud. 


### Préparation des données

#### Importation du fichier csv

Le fichier qui a été préalablement nettoyé coporte 36466 titres de nouvelles de presses publiées entre novembre 2017 et décembre 2021. Sa structure est très simple comme on peut le voir ci-dessous. 

```{r loadcsv, echo=TRUE}
df<-fread("data/corpus/media_benin.csv",encoding = "UTF-8")
kable(head(df))
```



#### Transformation en corpus quanteda

Pour faciliter l'analyse textuelle, le fichier d'origine est transformé en objet de type *corpus* pour être utilisé par le package d'analyse textuelle `quanteda`.  On ajoute au fichier différentes informations sur les périodes de temps pour faciliter les analyses ultérieures.

```{r create quanteda, echo=TRUE}
# library(quanteda)
# Create Quanteda corpus
qd<-corpus(df,text_field = "text")
qd$media<-as.factor(qd$who)
levels(qd$media)<-c("24 Heures","La Nouvelle Tribune")

qd$day     <- as.Date(qd$when)
qd$week    <- cut(qd$day, "weeks", start.on.monday=TRUE)
qd$month   <- cut(qd$day, "months")
qd$weekday <- weekdays(qd$day)


# Add global meta
meta(qd,"meta_source")<-"Media Cloud "
meta(qd,"meta_time")<-"Téléchargé le 18 décembre 2021"
meta(qd,"meta_author")<-"Auteur :  Claude Grasland"
meta(qd,"project")<-"Ecole d'été CIST 2022"

class(qd)
summary(qd,6)
head(qd,6)

```

#### Transformation en tibble

L'objet quanteda étant complexe, on peut décider à tout moment d'opérer une transformation inverse e se servant de la fonction *tidy* du package `tidytext`. 

```{r, echo=TRUE}
td <- tidy(qd)
class(td)
```

#### Variations temporelles

Avant d'analyser les nouvelles, il faut s'assurer que leur production est régulière dans le temps, à la fois pour l'ensemble de la période et au cours des différents jours de la semaine.

```{r, echo=FALSE}

news_weeks <- td %>% group_by(week,media) %>%
                    count(nbnews = n())
p<-ggplot(news_weeks, aes(x=as.Date(week),y=nbnews, col=media))+
   geom_line()+
   geom_smooth(method = 'loess', formula = 'y~x')+
   scale_y_continuous("Nombre de nouvelles", limits = c(0,NA)) +
   scale_x_date("Semaine") +
         ggtitle(label ="Nombre de nouvelles envoyées par flux RSS",
                  subtitle = "1er Nov.2017- 18 Dec.2021")
p
```
- *Commenaire* : Les deux journaux connaissent un régime assez régulier mais en croissance au cours du temps. On note toutefois une interruption de la *Nouvelle Tribune* entre mars et juin 2019. qui pourrait avoir été lien à l'interdiction temporaire du journal. 



```{r news_weekdays_fr, echo=FALSE}
#compute frequencies by weekday
#news_weekdays<-dt[,.(newstot=.N),by=.(weekday,who)]
news_weekdays <- td %>% group_by(weekday,media) %>%
                    count(nbnews = n()) %>% 
                   group_by(media) %>%
                   mutate(pct = 100*nbnews / sum(nbnews))


# Translate weekdays in english and order
news_weekdays$weekday<-as.factor(news_weekdays$weekday)
levels(news_weekdays$weekday)<-c("7.Dimanche","4.Jeudi","1.Lundi","2.Mardi","3.Mercredi","6.Samedi","5.Vendredi")
news_weekdays$weekday<-as.factor(as.character(news_weekdays$weekday))
news_weekdays<-news_weekdays[order(news_weekdays$weekday),]


p<-ggplot(news_weekdays, aes(x=weekday,fill = media, y=pct))+
         geom_bar(position = "dodge", stat="identity")+
         scale_x_discrete("Jour de la semaine")+
         scale_y_continuous("Part des nouvelles (%)", limits = c(0,NA)) +
         ggtitle(label ="Variations hebdomadaires des nouvelles",
                  subtitle = "1er Nov.2017- 18 Dec.2021")
p
```

- *Commenaire* : Les deux journaux connaissent clairement une baisse de leur activité au cours du week-end, l'émission de nouvelles par leur flux RSS  étant plus faible le samedi et le dimanche.



### Taggage géographique

Bien qu'il s'agissent de textes courts, les titres des nouvelles de presse comportent souuvent un grand nombre de noms de lieux qu'il était intéressant d'indetifier sous forme de "tags" c'est-à-dire d'étiquettes indiquant pour chaque nouvelle les lieux mentionnés. 



#### Dictionnaire

On peut par exemple repérer les pays étrangers qui sont cités dans les nouvelles en se servant d'un dictionnaire de mots ou de racines créé dans le cadre de projets de recherche sur les nouvelles internationales (ANR Geomedia, H2020 ODYCCEUS).

```{r load_dict, echo=FALSE}
# Load multilanguage dictionary
dict<-fread("data/corpus/global_state_V2.csv")

# Select french dictionary
dict <- dict[dict$lang=="fr",]

# Exclude Benin
dict<-dict[dict$ISO3 !="BEN",]


head(dict)
```



```{r func_annotate, echo=FALSE}
extract_tags <- function(qd = qd,                      # the corpus of interest
                         lang = "fr",                  # the language to be used
                         dict = dict,                  # the dictionary of target 
                         code = "ISO3" ,                # variable used for coding
                         alias = "x",                   # variable used for alias
                         tagsname = "states",           # name of the tags column
                         split  = c("'","’","-"),       # split list
                         tolow = TRUE  ,                # Tokenize text
                         comps = c("Afrique du sud")  # compounds
                         )
{ 


  
# Tokenize  
x<-as.character(qd)


if(length(split) > 0) { reg<-paste(split, collapse = '|')
                       x <- gsub(reg," ",x)}  
if(tolow) { x <- tolower(x)} 
toks<-tokens(x)

# compounds
if(length(split) > 0) { reg<-paste(split, collapse = '|')
                       comps<- gsub(reg," ",comps)}  
if(tolow)       {comps <- tolower(comps)}  
toks<-tokens_compound(toks,pattern=phrase(comps))

  
# Load dictionaries and create compounds

  ## Target dictionary

labels <-dict[[alias]]
if(length(split) > 0) { reg<-paste(split, collapse = '|')
                       labels<- gsub(reg," ",labels)}  
if(tolow)       {labels <- tolower(labels)}  
toks<-tokens_compound(toks,pattern=phrase(labels))
  
 # create quanteda dictionary
keys <-gsub(" ","_",labels)
qd_dict<-as.list(keys)
names(qd_dict)<-dict[[code]]
qd_dict<-dictionary(qd_dict,tolower = FALSE)

# Identify geo tags (states or reg or org ...)
toks_tags <- tokens_lookup(toks, qd_dict, case_insensitive = F)
toks_tags <- lapply(toks_tags, unique)
toks_tags<-as.tokens(toks_tags)
list_tags<-function(x){res<-paste(x, collapse=' ')}
docvars(qd)[[tagsname]]<-as.character(lapply(toks_tags,FUN=list_tags))
docvars(qd)[[paste("nb",tagsname,sep="")]]<-ntoken(toks_tags)



# Export results
return(qd)
}

### Application de la fonction
frcomps <-c("océan indien","continent américain", "pays américain*")

qd <- extract_tags (qd = qd,
                     lang="fr",
                     dict = dict,
                     code = "ISO3",
                     alias = "x",
                     tagsname = "states",
                     split = c("'","’","-"),
                     comps = frcomps,
                     tolow = TRUE)




```


#### Résultats du taggage

On regarde pour chaque journal la distribution des nouvelles en fonction du nombre de pays étrangers cités. Il apparaît alors clairement que *La Nouvelle Tribune* est beaucoup plus tourné vers l'international (39% des nouvelles citent au moins un pays étrangers) que *24 heures* (moins de 5% des nouvelles citent un pays étranger. 


```{r check_states1_news, echo=FALSE}
x<-100*prop.table(table(qd$nbstates,qd$media),2)
kable(x,digits=3,caption = "Nombre de pays étrangers cités dans les nouvelles")
```

On peut afficher les nouvelles qui affichent un nombre record de pays étrangers dans leur titre. 

```{r check_states2_news, echo=FALSE}
table(qd$nbstates)
check<-corpus_subset(qd,nbstates>3)
x<-data.frame(who=check$who,when = check$when,text=as.character(check),states=check$states,nbstates=check$nbstates)
x<-x[order(x$nbstates,decreasing = T),]
kable(head(x,10))
```






### Taggage thématique

On peut également procéder à un taggae thématique en cherchant à repérer les nouvelles qui correspondent à un sujet dont l'on souhaite étudier la présence dans les médias. A titre d'exemple, nous allons ici essayer de repérer les nouvelles traitant d'épidémies et de pathologies transmissibles

#### Dictionaire

Nous proposons par exemple de partir du dictionaire suivant :

```{r dico pandemic, echo=FALSE}
label <- c("épidémie*", "pandémie*", "crise sanitaire","virus", "vaccin*", "oms", "ébola", "ebola",  "h1n1","sras", "chikungunya", "choléra", "peste", "paludisme", "covid*","coronavir*","ncov*")
code <- c("epi", "epi", "epi","virus", "vaccin","oms", "ebola", "ebola",  "h1n1","sras", "chik", "chol", "pest", "palu","covid","covid","covid")
lang  <- rep("fr", length(label))
dict_pande <- data.frame(code,lang,label)
kable(dict_pande)

frcomps<-c("virus informatique")
```
```{r, echo=FALSE}
### Application de la fonction
frcomps <-c("virus informatique")


qd <- extract_tags (qd = qd,
                     lang="fr",
                     dict = dict_pande,
                     code = "code",
                     alias = "label",
                     tagsname = "pand",
                     split = NULL,
                     comps = frcomps,
                     tolow = TRUE)




```


#### Résultats du taggage


```{r check_pand1_news, echo=FALSE}
x<-100*prop.table(table(qd$nbpand,qd$media),2)
kable(x,digits=3,caption = "Part des nouvelles parlant de pandémies")
```

On peut afficher les nouvelles qui affichent un nombre record de pays étrangers dans leur titre. 

```{r check_pand2_news, echo=FALSE}
table(qd$nbpand)
check<-corpus_subset(qd,nbpand>1)
x<-data.frame(who=check$who,when = check$when,text=as.character(check),pand=check$pand, nbpand=check$nbpand)
x<-x[order(x$nbpand,decreasing = T),]
kable(head(x,10))
```

### Exploration multidimensionnelle

Une fois établi un double taggage géographique (Where) et thématique (What) on peut croiser ces deux dimensions avec le média qui a produit les nouvelles (Who) et le moment où les nouvelles ont été publiés (When). On dispose ainsi de quatre dimensions autour desquelles on peut agréger les nouvelles.


#### Transformation en hypercube

On commence par croiser les quatre dimensions afin de compter le nombre de nouvelles correspondantes et de stocker le résultat dans un objet unique appelé *hypercube*. 

```{r, echo=FALSE}

#' @title create an hypercube
#' @name hypercube
#' @description create a network of interlinked states
#' @param corpus a corpus of news in quanteda format
#' @param who the source dimension
#' @param when the time dimension
#' @param timespan aggreation of time
#' @param what a list of topics
#' @param where1 a list of states
#' @param where2  a list of states


hypercube   <- function( corpus = qd,
                        who = "source",
                        when = "when",
                        timespan = "week",
                        what = "what",
                        where1 = "where1",
                        where2 = "where2")
{


  
# prepare data

  don<-docvars(corpus)
  
  df<-data.table(id     = docid(corpus),
                 who    = don[[who]],
                 when   = don[[when]],
                 what   = don[[what]],
                 where1 = don[[where1]],
                 where2 = don[[where2]])

  # adjust id
 df$id<-paste(df$id,"_",df$order,sep="")
 
# change time span
  df$when<-as.character(cut(as.Date(df$when), timespan, start.on.monday = TRUE))

# unnest where1
  df$where1[df$where1==""]<-"_no_"
  df<-unnest_tokens(df,where1,where1,to_lower=F)
  
# unnest where2
  df$where2[df$where2==""]<-"_no_"
  df<-unnest_tokens(df,where2,where2,to_lower=F) 
  
# unnest what
  df$what[df$what==""]<-"_no_"
  df<-unnest_tokens(df,what,what,to_lower=F) 
  


# Compute weight of news
  newswgt<-df[,list(wgt=1/.N),list(id)]
  df <- merge(df,newswgt, by="id")


# ------------------------ Hypercube creation --------------------#
  
  
# Aggregate
  hc<- df[,.(tags = .N, news=sum(wgt)) ,.(who, when,where1,where2, what)]
  
# Convert date to time
  hc$when<-as.Date(hc$when)
  
# export
  return(hc)
  
}

### Application

hc <-    hypercube( corpus   = qd,
                    who      = "who",
                    when     = "when",
                    timespan = "day",
                    what     = "pand",
                    where1   = "states",
                    where2   = "states")
kable(head(hc))

```

Nous allons ensuite croiser les dimensions deux à deux et procéder à des test statistiques permettant de repérer les anomalies les plus remarquables à l'aide d'un test du chi-2.



```{r, echo=FALSE}
#### ---------------- testchi2 ----------------
#' @title  Compute the average salience of the topic and test significance of deviation
#' @name what
#' @description create a table and graphic of the topic
#' @param tabtest a table with variable trial, success and null.value
#' @param minsamp : Threshold of sample size requested for salience computation
#' @param mintest : Threshold of estimated value requested for chi-square test


testchi2<-function(tabtest=tabtest,
                   minsamp = 20,
                   mintest = 5) 
{
  tab<-tabtest
  n<-dim(tab)[1]
  
  # Compute salience if sample size sufficient (default : N>20)
  tab$estimate <-NA
  tab$salience <-NA
  tab$chi2<-NA
  tab$p.value<-NA
  if (tab$trial > minsamp){ tab$estimate<-round(tab$success/tab$trial,5)
  tab$salience<-tab$estimate/tab$null.value
  
  # Chi-square test if estimated value sufficient (default : Nij* > 5)
  
  for (i in 1:n) {
    if(tab$trial[i]*tab$null.value[i]>=mintest) {  
      test<-prop.test(x=tab$success[i],n=tab$trial[i], p=tab$null.value[i], 
                      alternative = "greater")
      tab$chi2[i]<-round(test$statistic,2)
      tab$p.value[i]<-round(test$p.value,5)
    } 
  }
  }
  return(tab)
}

```




#### Valeur de référence

On commence par calculer la valeur de référence qui est la part des nouvelles relatives à la thématique retenue.

```{r, echo=FALSE}
### ---------------- what ----------------
#' @title  Compute the average salience of the topic
#' @name what
#' @description create a table and graphic of the topic
#' @param hc an hypercube prepared as data.table
#' @param subtop a subtag of the main tag (default = NA)
#' @param title Title of the graphic


what <- function (hc = hypercube,
                  subtop = NA,
                  title = "What ?")
{
 
  
tab<-hc
if (is.na(subtop)){tab$what <-tab$what !="_no_"}else {tab$what <- tab$what == subtop}

tab<-tab[,list(news = sum(news)),by = what]
tab$pct<-100*tab$news/sum(tab$news)

p <- plot_ly(tab,
             labels = ~what,
             values = ~pct,
             type = 'pie') %>%
  layout(title = title,
         xaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE),
         yaxis = list(showgrid = FALSE, zeroline = FALSE, showticklabels = FALSE))

output<-list("table" = tab, "plotly" =p)

return(output)

}



### Example


res_what <- what(hc = hc,
             subtop = NA,
             title = "Valeur de référence (What)")
res_what$table
res_what$plotly
```

- **Commentaire** : Sur l'ensemble de la période, la part des nouvelles du corpus traitant de pandémies est de 5.5%






#### Variation selon le journal (Who.What)

On examine ensuite si cette valeur de référence est la même dans les différents journaux du corpus.

```{r, echo=FALSE}

#### ---------------- who.what ----------------
#' @title  visualize variation of the topic between media
#' @name who.what
#' @description create a table of variation of the topic by media
#' @param hc an hypercube prepared as data.table
#' @param test : visualize test (TRUE) or salience (FALSE)
#' @param minsamp : Threshold of sample size requested for salience computation
#' @param mintest sample size of estimate for chi-square test (default = 5)
#' @param title Title of the graphic


who.what <- function (hc = hypercube,
                      test = FALSE,
                      minsamp = 20,
                      mintest = 5,
                      title = "Who says What ?")
{
  
  tab<-hc
  {tab$what <-tab$what !="_no_"}
  
  tab<-tab[,list(trial = sum(news),success=round(sum(news*what),0)),by = list(who)]
  ref <-round(sum(tab$success)/sum(tab$trial),4)
  tab$null.value<-ref
  
  tab<-testchi2(tabtest=tab,
                minsamp = minsamp,
                mintest = mintest)
  
  
  
  if (test==FALSE) {tab$index =tab$salience
  tab<-tab[tab$trial > minsamp,]
  mycol<-brewer.pal(7,"YlOrRd")
  } 
  else {tab$index=tab$p.value
  tab<-tab[tab$trial*tab$null.value>mintest,]
  mycol<-brewer.pal(7,"RdYlBu")
  mycol[4]<-"lightyellow"
  }
  
  p <- plot_ly(tab,
               x = ~who,
               y = ~estimate*100,
               color= ~index,
               colors= mycol,
               hoverinfo = "text",
               text = ~paste('Source: ',who,
                             '<br /> Total news  : ', round(trial,0),
                             '<br /> Topic news : ', round(success,0),
                             '<br /> % observed  : ', round(estimate*100,2),'%',
                             '<br /> % estimated : ', round(null.value*100,2),'%',
                             '<br /> Salience : ', round(salience,2),  
                             '<br /> p.value : ', round(p.value,4)),
               type = "bar")  %>%
    layout(title = title,
           yaxis = list(title = "% news"),
           barmode = 'stack')
  
  output<-list("table" = tab, "plotly" =p)
  
  return(output)
  
}


### Example



res_who_what<- who.what(hc=hc, 
                        test = TRUE,
                        minsamp = 5,
                        mintest = 1,
                        title = "Variation selon la source (Who.What)")
res_who_what$plotly

```


- **Commentaire** : Il existe une différence significative entre les deux journaux (p<0.001). Moins de 2% des nouvelles publiées par *24 Heures* parlent de pandémie contre plus 7% des nouvelles publiées par *La Nouvelle Tribune*.


#### Variation temporelle (When.What)

Comment évolue l'intérêt des journaux pour la thématique au cours du temps. La période observée est évidemment marquée par une très forte discontinuité avec l'arrivée du Covid-19. Mais quel est le calendrier précis de son impact sur la presse du Bénin ?

```{r, echo=FALSE}
#### ---------------- when.what ----------------
#' @title  visualize variation of the topic through time
#' @name when.what
#' @description create a table of variation of the topic by media
#' @param test : visualize test (TRUE) or salience (FALSE)
#' @param minsamp : Threshold of sample size requested for salience computation
#' @param mintest sample size of estimate for chi-square test (default = 5)
#' @param title Title of the graphic


when.what <- function (hc = hypercube,
                       test = FALSE,
                       minsamp = 20,
                       mintest = 5,
                       title = "Who says What ?")
{
  
  tab<-hc
  {tab$what <-tab$what !="_no_"}
  
  tab<-tab[,list(trial = sum(news),success=round(sum(news*what),0)),by = list(when)]
  ref <-round(sum(tab$success)/sum(tab$trial),4)
  tab$null.value<-ref
  
  tab<-testchi2(tabtest=tab,
                minsamp = minsamp,
                mintest = mintest)
  
  if (test==FALSE) {tab$index =tab$salience
  tab<-tab[tab$trial > minsamp,]
  mycol<-brewer.pal(7,"YlOrRd")
  } 
  else {tab$index=tab$p.value
  tab<-tab[tab$trial*tab$null.value>mintest,]
  mycol<-brewer.pal(7,"RdYlBu")
  mycol[4]<-"lightyellow"
  }
  
  
  p <- plot_ly(tab,
               x = ~as.character(when),
               y = ~estimate*100,
               color= ~index,
               colors= mycol,
               hoverinfo = "text",
               text = ~paste('Time: ',when,
                             '<br /> Total news  : ', round(trial,0),
                             '<br /> Topic news : ', round(success,0),
                             '<br /> % observed  : ', round(estimate*100,2),'%',
                             '<br /> % estimated : ', round(null.value*100,2),'%',
                             '<br /> Salience : ', round(salience,2),  
                             '<br /> p.value : ', round(p.value,4)),
               type = "bar")  %>%
    layout(title = title,
           yaxis = list(title = "% news"),
           barmode = 'stack')
  
  output<-list("table" = tab, "plotly" =p)
  
  return(output)
  
}


# Modify time period by month
hc2 <- hc %>% mutate(when = cut(when,breaks="month"))


res_when_what<- when.what(hc=hc2, 
                          test=TRUE,
                          minsamp=5,
                          mintest=1,
                          title = "Variation temporelle (When.What)")


res_when_what$plotly
```
- **Commentaire** : Comme dans l'ensemble des pays du Monde, on observe un pic maximal en mars-avril 2020 avec plus de 20% des nouvelles parlant de pandémie contre moins de 1% au cours de l'année précédente. Passée cette période d'intérêt maximal, on observe une stabilisation aux alentours de 7 à 10% des nouvelles publiées. 



#### Variations spatiales (Where.What)

Une pandémie étant un phénomène à la fois spatial et temporel, il est intéressant de repérer les pays cités dans les nouvelles quii parlent de pandémie et de repérer les pays qui y ont été associé le plus significativement. 

```{r, echo=FALSE}

#### ---------------- where.what ----------------
#' @title  visualize spatialization of the topic 
#' @name where.what
#' @description create a table of variation of the topic by media
#' @param hc an hypercube prepared as data.table
#' @param test : visualize test (TRUE) or salience (FALSE)
#' @param minsamp : Threshold of sample size requested for salience computation
#' @param mintest sample size of estimate for chi-square test (default = 5)
#' @param map a map with coordinates in lat-long
#' @param proj a projection accepted by plotly
#' @param title Title of the graphic


where.what <- function (hc = hypercube,
                        test = FALSE,
                        minsamp = 20,
                        mintest = 5,
                        map = world_ctr,
                        proj = 'azimuthal equal area',
                        title = "Where said What ?")
{
  
  tab<-hc
  tab$what <-tab$what !="_no_"
  
  tab<-tab[,list(trial = round(sum(news),0),success=round(sum(news*what),0)),by = list(where1)]
  ref <-round(sum(tab$success)/sum(tab$trial),4)
  tab$null.value<-ref
  
  tab<-testchi2(tabtest=tab,
                minsamp = minsamp,
                mintest = mintest)
  
  
  
  tab<-tab[order(-chi2),]
  
  
  
  if (test==FALSE) {tab$index =tab$salience
  tab<-tab[tab$trial > minsamp,]
  mycol<-brewer.pal(7,"YlOrRd")
  } 
  else {tab$index=tab$p.value
  tab<-tab[tab$trial*tab$null.value>mintest,]
  mycol<-brewer.pal(7,"RdYlBu")
  mycol[4]<-"lightyellow"
  }
  
  
  map<-merge(map,tab,all.x=T,all.y=F,by.x="ISO3",by.y="where1")
  
  
  
  #map2<-map[is.na(map$pct)==F,]
  #map2<-st_centroid(map2)
  #map2<-st_drop_geometry(map2)
  
  
  g <- list(showframe = TRUE,
            framecolor= toRGB("gray20"),
            coastlinecolor = toRGB("gray20"),
            showland = TRUE,
            landcolor = toRGB("gray50"),
            showcountries = TRUE,
            countrycolor = toRGB("white"),
            countrywidth = 0.2,
            projection = list(type = proj))
  
  
  
  p<- plot_geo(map)%>%
    add_markers(x = ~lon,
                y = ~lat,
                sizes = c(0, 250),
                size = ~success,
                #             color= ~signif,
                color = ~index,
                colors= mycol,
                hoverinfo = "text",
                text = ~paste('Location: ',NAME,
                              '<br /> Total news  : ', round(trial,0),
                              '<br /> Topic news : ', round(success,0),
                              '<br /> % observed  : ', round(estimate*100,2),'%',
                              '<br /> % estimated : ', round(null.value*100,2),'%',
                              '<br /> Salience : ', round(salience,2),  
                              '<br /> p.value : ', round(p.value,4))) %>%
    
    layout(geo = g,
           title = title)
  
  
  
  output<-list("table" = tab, "plotly" =p)
  
  return(output)
  
}




### Example

map<-readRDS("data/corpus/world_ctr_4326.Rdata")
hc2<-hc %>% filter(where1 !="_no_", where2 !="_no_")

res_where_what<- where.what(hc=hc2,
                            test=TRUE,
                            minsamp=10,
                            map = map, 
                            mintest =1,
                            title = "Variation spatiale (When.What)")
res_where_what$plotly
```
- **Commentaire** : 6.7%  nouvelles qui citent au moins un pays étranger parlent de pandémie ce qui est plus que la moyenne de référence qui était de 5.5% de nouvelles parlant de pandémie. Certains pays sont plus fréquemment associés à la pandémie que d'autres. Ainsi près de 19% des nouvelles parlant de Chine parlent de pandémie au cours de la période d'observation. Il en va de même pour le Royaume-Uni (15.7%), l'Italie (13.4%), l'Inde (11.9%) ou la Russie (10.9%). La France, en revanche, n'est pas spécialement associée à la pandémie (6.3% des nouvelles) même si elle est le pays le plus cité à ce sujet en nombre absolu. Elle demeure en effet présente dans un grand nombre d'autres sujets (économie, sport, culture, politique, ...). Il en va de même pour les Etats-Unis (6.2%).Finalement, d'autres pays sont significativement peu associés à la pandémie comme la Côte d'Ivoire (2.9%) ou le Nigéria (0%).



#### Prolongements possibles

Au lieu de repérer les pays étrangers, on pourrait essayer de repérer les communes ou département du Bénin qui sont cités dans les nouvelles. Ce qui supposerait de construire un nouveau dictionnaire. On peut évidemment choisir par ailleurs d'autres thèmes que celui des pandémies. 




# Bibliographie {-}

<div id="refs"></div>


# Annexes {-}


## Infos session  {-}

```{r session_info, echo=FALSE}
kableExtra::kable_styling(kable(sessionRzine()[[1]], row.names = F))
kableExtra::kable_styling(kable(sessionRzine()[[2]], row.names = F))
```





## Citation {-}

```{r generateBibliography, echo=FALSE}

cat(readLines('cite.bib'), sep = '\n')

``` 

<br>

## Glossaire {- #endnotes}

```{js, echo=FALSE}

$(document).ready(function() {
  $('.footnotes ol').appendTo('#endnotes');
  $('.footnotes').remove();
});

```


